#+TITLE: A Primer

This is largely an aid to my own memory so bear with me.

* The Syntax
  I'll start by describing the syntax. Not because it's the most important nor
  the best place to start, but because it's the first thing you see.

  The syntax is, for the most part, inspired by Clojure. In particular, the
  literal data structures: =[]=, ={}=, & =#{}= mean what you expect.

  One major goal is that if an expression *looks* familiar, then it should act in
  a familiar way. The degree to which I've met that goal is an open question,
  but things are still in flux.

  #+BEGIN_SRC clojure
    (def f (fn [x y] (+ x y)))
    ;; => f

    (f 4 5)
    ;; => 9

    (if (< (f x y) 0) (g x) (h y))
  #+END_SRC

  and so on.
** Pairs vs Cons Cells
   I wanted to avoid cons cells entirely in this language, but I'm starting to
   realise why they're important: the symmetry between calling a function as (f
   x y z) and defining it as (fn [x y z] ...). The fact that the tail of the
   cons cell (f x y z) is the cons cell (x y z) makes this automatic. So... if
   I want to be able to call things with a single arg that isn't necessarily a
   list, I need improper cons cells. Once we have that, the convention of
   multiple args becomes automatic.

   That said, there are no cons *lists*. The linked list was once a great idea,
   but is, on modern hardware, just about the worst possible way to store
   data.

   Lisp uses cons lists in two ways: 1) as lists of data, for which we'll just
   use vectors, and 2) as a syntax for invoking functions.

   (f x y z) means eval the head (f), eval the tail (x y z), then apply the
   latter to the former.

   We will keep the (f x y z) syntax for calling functions, but instead of
   (cons f (cons x (cons y (cons z nil)))), we store it as (pair f [x y z]).

   This (f x y z) is identical to (f . [x y z]).

   The =.= can be interpreted in a functional sense as =apply=, or in an actor
   sense as =send=. It turns out they're both equivalent with the right
   underlying abstractions.
** No Implicit Evaluation
   In constrast to most lisps (in fact all I've ever found save one called
   [[https://web.cs.wpi.edu/~jshutt/kernel.html][Kernel]]), we have no implicit evaluation. That means that both arguments and
   return values are passed by name. That sounds crazy. If not for the Kernel
   thesis I'd never have believed such a scheme could work, but it does!

   To explain, I'll need to jump ahead to μ.
** The μ Operator
   Why μ? Two reasons.

   1) it's the next letter after λ.
   2) It looks weird, and I want things that don't act as expected to look
      abnormal.

   μ defines pure syntax operations.

   Example:

   #+BEGIN_SRC clojure
     (def f (μ [x y] (+ x y)))
     ;; => f

     (f 4 5)
     ;; => (+ x y)

     ;; which is an error because x and y aren't defined.
   #+END_SRC

   μ doesn't define a macro. A μ is first class: they can be passed and returned
   as values, they operate at runtime (or compile time, or anytime).

   But a μ has no direct access to the environment (nor the continuations). You
   can pass a namespace to a μ (it's just a map) and take it apart so as to
   perform any reflective feat your heart may desire, but the lexical environment
   of an expression is immutable and the expression must be redefined to change
   it, so there's nothing that can be done by the body of a μ with its own
   lexical environment.

   Also remember that macros evaluate their own output (or properly speaking the
   compiler expands the macro and then calls =eval= on the result), whereas μs do
   not.

   But if nothing evaluates the output of a μ, how do they *do* anything?
** Immediate Evaluation
   The answer to the hanging question of the last section is that there is a
   syntactic expression that "means" =eval=.

   In reality it means "evaluate the following expression as soon as you have
   all of the information required to do so", which will remain ambiguous until
   we get into the details of the AST.

   That syntax is the tilde =~=.

   #+BEGIN_SRC clojure
     (def f (μ [x y] (+ ~x ~y)))
     ;; => f

     (f 4 5)
     ;; => (+ 4 5)

     ;; `~x` and `~y` can be evaluated as soon as `f` is called. Since they're
     ;; bound in the lexical environment of the body, eval reduces to lookup.
     ;; But remember that the output is not evaluated, so `f` returns an expression.

     (def g (μ [x y] ~(+ x y)))
     ;; => g

     (g 4 5)
     ;; => 9

     ;; Note that this only works because `+` is defined so as to explicitly eval
     ;; its arguments before applying the underlying addition operation.
   #+END_SRC

   Given a function that only works on literal values (say a version of =+=
   called =+*=), we can define an operator =wrap= which will invoke it in the
   standard applicative manner by first evaluating its arguments:

   #+BEGIN_SRC clojure
     (def wrap
       (μ f
          ~(μ args
              ~(~f . ~~args))))

     (def + (wrap . +*))
   #+END_SRC

   We can even define λ (here called =fn=) as a μ which receives arguments,
   evaluates them, passes the evaluated values to another μ and then evaluates
   its output:

   #+BEGIN_SRC clojure
     (def fn
       (μ [params body]
          ~(μ args
              ~((μ ~params ~~body) . ~~args))))
   #+END_SRC

   This is the actual definition of =fn= at the core of the language.

   It may help to think of the body of a μ as quasiquoted, except that we can
   unquote as many times as we please.

   That said, there is no quote, unquote, quasiquote, eval, nor apply defined in
   the language — you can easily write them yourself, but please don't — so the
   analogy is a bit weak.
** An Exception
   One final note: I said above that there is *no* implicit evaluation. You've
   probably noticed in my examples that everything typed into the repl *is*
   automatically evaluated. The same holds for the code reader =loadfile=. This
   is a convenience that once again makes code that acts as expected look
   normal. Also there's no point entering code into a repl unless you want it to
   be evaluated, so it seems like the right thing to do.

   μs are, of course, free to evaluate their arguments when and as they see fit.
   =def= adds an evaluation to the beginning of its body, for instance. I think
   that's the right thing to do as well, but it adds a bit of asymmetry to the
   definition of metaprogramming operations like =fn= and =wrap=.
* Reduction and the AST
  As covered in the previous section, whether or not an expression will ever be
  evaluated — that is whether a given expression is code or data — is expressed
  syntactically. Given =(f x)= the question of whether =x= will or will not be
  evaluated doesn't depend on the *type* of =f=, but on its syntactic
  representation.

  Syntax is topological. But it has no notion of time. The question of *whether*
  an expression will be evaluated is syntactic, but the question of *when* an
  expression will be evaluated is contextual: it depends on the unknowns in the
  expression and on whoever provides them.

  Take for example, the humble =λ=. Copying from above, we have:

   #+BEGIN_SRC clojure
   (def fn
     (μ [params body]
        ~(μ args
            ~((μ ~params ~~body) . ~~args))))
   #+END_SRC

   Notice that everything here is a symbol, even =def= and =μ=. This expression
   can only be evaluated in an environment that provides meaning for those
   symbols. And yes, you can override them if you want to. You can also rebind
   =self-insert-command= if you feel like it.

   Some notes on abstract syntax: =(f x y)= is a Pair, in our parlance. The head
   is the ~symbol~ =f= and the tail is the list (of symbols) =[x y]=.

   A Pair is just data. It doesn't represent a function application. An
   Application is represented by =#(...)= to distinguish it from a pair.

   =#(f x y)= represents the application of =[x y]= to the *symbol* =f=, which
   is senseless and would result in error.

   In a traditional lisp, we have
   =(eval (f . tail))= => =(apply (eval f) (eval tail))=.

   But we aren't writing an applicative language, so we want something more
   like:

   =(eval (f . tail))= => =(apply (eval f) tail)=

   Which is represented syntactically by our first rule:

   =~(f . tail)= => =#(~f . tail)=

   Thus ~(f x y) -> #(~f x y) which applies [x y] to whatever the symbol =f= is
   bound to. The μ to which f is bound, might in turn choose to evaluate
   (lookup) x and y to do something with them.

   The second thing we need to do is create actual μ objects. I don't believe
   its possible to implement μ in this language, so we need an escape hatch
   which I've taken to calling primitive macros.

   Let =μ= be bound to the primitive =#Pmac[createμ]= then we have

   =~(μ params body)= => =#(#Pmac[createμ] params body)=

   which causes the underlying implementation of the primitive =createμ= to be
   called with the tail of the application: =[params body]= as well as the
   current environment and continuation map.

   It's up to a primitive macro to continue appropriately, which makes them very
   brittle. As of yet I only need 4 of them. They're all related to manipulating
   context. Notably all control flow can be implemented as normal μs in the
   language itself.

   There are also primitive functions which can only be applied to literal
   values. These aren't essential to having a working language, but while you
   could implement arithmetic lambda calculus style, it's advisable to wrap the
   machine's arithmetic capabilities (or those of the implementation language at
   least).

   Primitive functions are best thought of as functions implemented in another
   language. We'll come back to them later.

   Our third rule of execution involves what it means to apply arguments to a μ.

   If we let =(#μ params body)= represent an actual μ object with =params= and
   =body=, then we can write:

   =#((#μ params body) . args)= =>
   =(extend-enviroment (destructuring-bind params args) body)=

   Note that =extend-environment= and =destructuring-bind= are only defined in
   the implementation at present. Bootstrapping is not a short term goal.

   This is just Steele's "λ the ultimate declarative" without applicative
   semantics. A μ takes arguments, binds them to formal parameters, and then
   reduces its body in the new, extended environment.

   N.B.: "reduction" in this context refers to applying the rules we're laying
   down. They simplify an expression without necessarily evaluating it. If
   there's sufficient information, then reduction will fully interpret an
   expression, but in general there will be arguments that aren't known yet.

   There's a special case of applying arguments which is just β-reduction:

   =#((#μ X Y) . ~Z)= (where X, Y, & Z stand for subexpressions)

   becomes Y with symbols occuring in X renamed to their bindings in Z.

   In particular =#((#μ X Y) . ~X)= => =Y=. This actually comes up a bunch when
   compiling, so it's important to recognise.

   Finally, eval distributes over lists, and literal values are fixed points to
   eval.

   That is: =~V= => =V= if V is a literal value (number, string, bool, etc.)

   and: =~[X Y Z]= => =[~X ~Y ~Z]=
** Summary
   Reduction is the process of applying the following rules repeatedly until
   none can be applied any longer:

   An expression is considered "fully reduced" if it contains no immediate or
   application nodes.

   1) ~(X . Y) <=> #(~X . Y)
   2) #(#Pmac[f] . tail) => always escapes to implementation
   3) #(#Pfn[f] . tail) => invokes foreign code iff tail is fully reduced.
   4) #((#μ X Y) . Z) => Y in the current lexical env extended by {X Z}
      where {X Z} represents the destructured binding of X to Z

      The essentially says the ~X inside Y means Z.
   5) #((#μ X Y) . ~Z) => Y with {X Z} as a renaming mask
      If ~X means ~Z then use of X stands for use of Z.

      The special case Z=X makes this a noop.
   6) ~V <=> V whenever V is a literal value
   7) ~[X Y ...] <=> [~X ~Y ...]
      Similar rules apply to maps and sets in the obvious fashion.

   Some of these rules are reversable, some are not because information is lost.

   The order in which these rules are applied is irrelevant since all
   permutations will result in the same final state (possibly with some
   backtracking over the reversible rules, I'm not sure).

   I have a handwavy proof that validates my intuition regarding the previous
   assertion, so be skeptical.

   This (indirectly) answers the question of when an immediate expression gets
   evaluated. By following the rules from a valid input, you will always end up
   at a value. That value might be a μ waiting on arguments. Obviously the
   interior of that μ will be blocked to some extent until those arguments are
   known.

   Note on implementation:

   The current interpreter implicitly embodies these rules. When I get around to
   writing a compiler, the first step will be an explicit search over these
   rules to find a "maximally reduced normal form" which can then be translated
   into lower level code. But that's for another day.
* The Environment
  When reflecting on code at runtime, as when debugging or working in a repl,
  you see sexps containing symbols. Each symbol is either statically defined by
  the lexical context at dev time, in which case it's tagged with a pointer to
  its referrant. Otherwise it's a local variable, that is a value bound by =let=
  or passing arguments to an operator, in which case it is tagged with a
  reference to the binding form. This is intended to make shadowing obvious.

  To the runtime, however, there are no symbols, only values. Values known but
  not yet operated upon and values as yet unknown.

  That needs a better explanation...


* Channels and Streams
  In this language channels and streams are two sides of the same phenomenon.

  A channel is a black box to which messages are sent. They take the place of
  addresses in actor languages or objects in OO languages. In a sense they are
  write only, but they never actually change.

  A stream is a lazy, potentially unbounded, sequential data structure. A stream
  is a value, but it might not yet be known. Reading the first element off of a
  stream will return it if it is known right now, or it will park and wait for a
  value if it isn't. Calling =rest= on a stream returns the stream consisting of
  all but the first element and never parks.

  The relationship is trivial: every channel has a corresponding stream. Writes
  to the channel realise the next element of the stream. Thus calling =first= on
  a stream is equivalent to taking from a channel in CSP, except that it will
  only park once; further calls will just return the known value.

  Sending a message to a channel where no one is reading the corresponding
  stream will also park. This provides backpressure on emissions and lets the
  runtime know that all messages sent from a given μ have been received before
  cleaning up and moving on.

  A stream is a pseudo value. It can be passed as an argument, but it cannot be
  emitted, hence cannot be returned.

  A channel/stream pair can be created explicitly within a ν. When passed as a
  value, the stream part is passed, when passed in a continuation map, the
  channel part is passed. This is confusing to explain, but syntactically it's
  quite natural.
* The Continuation Map and Emission
  One of the bigger differences between this language and standard lisp (or any
  von neumann style language with which I'm familiar) is that there is no single
  "current continuation", but instead a set of them. And control can pass to any
  number of them from the current expression. It can even pass to several of
  them several times each.

  This is not too different from an actor system, but messages are always sent
  in batches from tail position. The continuation map is akin to Erlang's
  "address book", but it's aggressively local and indirected. You can't send
  messages to named things, you send messages on named channels (whose name you
  choose), and something above you in the call graph binds those names to actual
  channels.

  The underlying theory comes from what Koestler called holarchy which is the
  idea that each unit is autonomous from its own point of view: it can set up
  subcomputations and connect the results of those to channels, but it's
  simultaneously subordinate to higher levels in that it doesn't ultimately know
  how its outputs will be used, nor whence its inputs come.

  A more programming friendly metaphor: Pure functions can't do anything at the
  end of the day. That's why we needed monads before we could create Haskell.

  Do blocks (progn for the die hard lispers) exist because we generally want our
  code to do things other than compute and return answers. These side effects
  are the source of all kinds of complexity. We can't live without them, but oh
  boy.

  So, I've made a change to the traditional structure: there are no progns.
  Function arguments are not evaluated in order. You can't control the sequence
  in which things happen at all. All you can do is establish data dependencies
  between computations and then send batches of messages.

  So if you want to add logging to a program, you can't just stick in a
  statement and have a side effect coming from nowhere. But you can intercept a
  batch of messages and add one more message to be sent to a logging channel.

  So we can still perform side effects, but those side effects must exist as
  pure data (messages) sent to the things that actually bang on the world. Those
  messages can be logged, replayed, or even filtered. That makes a big
  difference.

  The continuation map doesn't exist as a first class thing from within the
  language. Rather it is maintained by the runtime (interpreter presently) and
  can be modified by calling primitive macros. I don't want to write code in cps
  if I can avoid it.
  - comment [2025-03-31 Mon 10:38]
    the above isn't quite true any longer with νs

  When a batch of messages are emitted, they are all delivered (logically
  speaking) concurrently. If there's enough spare compute, they'll be delivered
  (execute) in parallel, otherwise they get queued. See the section on work
  stealing for details. Not being able to depend on order of delivery saves us a
  world of pain when it comes to starvation and race conditions.

  The only guarantees we make on delivery are

  1) messages are delivered strictly after they are sent
  2) multiple messages sent from A to B are delivered exactly once and in order

  Note that if A and C both send messages to B, then those streams can be
  interleaved in any fashion, but the subsets from A and C will be in the order
  sent.

  These are the only guarantees we can provide without pretending special
  relativity doesn't apply to us. For better or for worse, it does apply to us.

  In essence, a fully consistent totally ordered view of the world is only
  possible from a single point in space. Once you have multiple actors in
  different locations, perspective becomes an irreducible aspect of the system.

  So if you want a consistent view, put messages in a queue and process them in
  serial. It's not that hard.

  Now it's time to talk about our next special construct: ν.
* ν
  The above discussion of emission assumes that a μ is only ever called once.
  From a point of view internal to the μ, this is true, but from the point of
  view of the caller, it might not be.

  From a syntactic transform point of view, the application of arguments (a
  value) to a μ is replaced by the first value emitted by that μ to its return
  channel.

  A μ can emit multiple values to its return channel, but if invoked in the
  standard (f x) fashion, all but the first return value will be dropped into
  the void.

  A program is a network of value->value operators (μs) connected by channels
  into an arbitrary graph. The next question that you should ask is "how do we
  manipulate that graph?".

  The answer is a construct that I'm calling ν because it's the next letter
  after μ. These names are all tentative.

  A ν takes a value (which will generally contain one or more streams) and
  receives its continuation map bound to the second argument to ν.

  The syntactic interpretation of the application of a value to a ν is that the
  application is replaced by the *return stream* of the ν. The ν explicitly
  manages its own return channel, which lets us manipulate the message passing
  topology of the program directly.

  When an application of a μ to an argument is invoked, it is considered
  "finished" when all messages emitted have been *received*. Just ensuring that
  they are queued for delivery is insufficient, since we want to be able to
  sequence one thing after another. Requiring receipt also has the effect of
  applying backpressure on a network.

  When an application of a ν to an argument is invoked, it is considered
  finished when all subcomputations are finished.

  A ν cannot itself emit anything. Instead it defines a computation graph and
  the μs within that graph emit values.

  Thus the return stream of the ν must be passed in the continuation map to some
  μ is the ν is to produce anything for its caller.

  In order for a ν to create a computation graph, it needs to be able to invoke
  multiple things. Remember we don't have a =do= or =progn= expression.

  Thus we have three special operators which only make sense within within the
  body of a ν, these are =seq=, =conc=, and =connect=.

  =connect=, as the name implies, connects an operator (ν or μ) to input and a
  continuation map.

  =seq= takes an arbitrary number of connected operators and ensures that each
  runs after the previous has finished.

  =conc= takes an arbitrary number of connected operators and sets them to run
  concurrently.

  =conc= and =seq= can be nested. A =seq= is finised when its last operator is
  finished and a =conc= is finished when all of its operators are finished.

  Examples (finally):

  #+BEGIN_SRC clojure
    (defn map [f]
      (ν [xs] ccs
         (when (first xs)
           (seq
            (connect f (first xs) ccs)
            (connect (map f) (rest xs) ccs)))))

    (defn comp [f g]
      (ν [xs] ccs
         (let [inner (chan)]
           (conc
            (connect f xs (assoc ccs :return inner))
            (connect g inner ccs)))))
  #+END_SRC

  (map f) applies f to the first element of a stream and then applies (map f) to
  the rest of the stream. map is stateless, so this works just fine. We could
  also name the inner ν and use recursion but that isn't necessary here.

  The steps in map are =seq= because both processes are connected to the same
  return channel and we want to ensure that [x1 x2 ...] -> [(f x1) (f x2) ...],
  i.e. that order is preserved.

  Because it's stateless, =map= is a prime contender for parallelism. However,
  whether that is useful or not depends not just on the data being processed
  (compute per element vs overhead of distribution) but also on what happens
  downstream since if we're folding with a non associative operator we're just
  going to have to buffer everything while the fold catches up.

  In the case of =comp=, we compose operators concurrently because the inner
  channel connecting f to go provides a data dependency. g will park on reading
  (first inner) until f has emitted a value, but if =xs= is a stream and =f=
  transduces it onto inner, then f and g can operate in parallel at different
  steps of the computation.

  This approach is automatic pipeline parallelism. It will only actually happen
  in parallel if a work step is expensive or cpus are sitting idle so f or g are
  stolen by another executor, this should reduce overhead in practice. But that
  needs empirical confirmation.

  Since a ν returns its return stream, ((comp (map f) (map g)) xs) is equivalent
  to ((map g) ((map f) xs)), as it should be.

  This is a little ugly though because if f and g are μs, then (g (f x)) is
  (first ((comp f g) x)). Really comp should be the same type as g for this to
  work out. I haven't decided how to do that yet.
* Stateful computation
  Sometimes a computation needs to retain state across messages. Given that we
  have no mechanism by which to assign or mutate, how do we accomplish it?

  The easiest way would be to have a μ which from tail position sends a messages
  to its future self (plus whatever emissions it needs to make).

  This would be a perfect solution except for the fact that we have no control
  over the order of delivery, so how do we know that your future self will
  receive its new state before it receives its next input?

  Sadly you can't.

  This, ultimately, was the deciding factor in introducing the ν. Here's how we
  can create a stateful accumulator using ν:

  #+BEGIN_SRC clojure
    (defn accumulate [rf init]
      (ν [xs] ccs
         ;; a read on a last-chan always returns the last thing written. They are atomic.
         ;; Semantically it is just (last (chan)), but is builtin to make life easier.
         (let [state (last-chan init)]
           (seq
            ;; Using a state channel to pass output back to yourself is a common
            ;; pattern.
            ;; Here we reduce into the state channel without returning anything.
            (connect (map (fn [[state x]] (emit :state (rf state x))))
                     (combine state xs)
                     (assoc ccs :state state))
            ;; After the stream `xs` is exhausted, we return the head of the
            ;; accumulation's state stream.
            (connect identity (first state) ccs)))))

    (defn into [coll]
      (fn [xs]
        ;; Use `first` to get back to "fn space" from stream space.
        (first ((accumulate conj coll) in))))

    ((into []) ((comp (map inc) (filter even?)) [1 2 3 4 5]))
    ; => [2 4 6]
  #+END_SRC

  =accumulate= here returns a stream which will only ever realise one value. To
  make a practical API, we define =into= which has the behavious you would expect.

  The api is still pretty ugly in that we need so many extra parens. Once we
  have function overloading, that can be fixed easily. Is currying the answer? I
  don't think so. It will get ugly when we start calling functions with non
  sequential arguments: (f . {...}) for example. Plus we'd need to curry from
  right to left and there will likely be ambiguity to deal with.
* Outline
  Sections to be written.
** History, Environment, and Context
   I gave a [[https://www.youtube.com/watch?v=Kgw9fblSOx4][talk]] on some of these ideas a long time ago. My opinions on the
   benefits haven't changed, but I've found better ways to implement them.

   You can only have unique permalinks without global concensus if:

   1) You and the link author share some context
      At the very least you have to look in the right place.
   2) The links can grow in size arbitrarily
      Fixed size hashes will eventually lead to collision. I don't care how few
      atoms there are in the observable universe.
   3) Divergences can be converged without consulting either author
      If two people who've never heard of each other publish under the same hash
      at roughly the same time, anyone who knows of both must be able to
      "decorrupt" the hashmap by themselves in a way that anyone else with whom
      they communicate can understand and replicate. The original authors might
      be lightyears away so they cannot be relied upon. Even if they're right
      here, they can't be relied upon to agree.

   None of those criteria have anything to do with hashes.

   Hashes are useful for their concision and cease to be beneficial when you
   start to need sha512 to guarantee uniqueness.

   Using large hashes makes microreferences infeasible for the same reason that
   credit card fees make microtransactions infeasible.

   It turns out that if you keep a history of edits going all the way back to
   the beginning of the web, then you and the person you're sending to (or
   receiving from) can always go back to a shared context and regrow from there.

   In general you don't even need to go back very far if you communicate with
   this other person frequently.

   So my approach is to never throw away history. Spool it out to disk or other
   durable storage if you don't need it in ram (and you don't, mostly) but don't
   throw it away.

   Some sort of compression will be required eventually. Kafka has a decent
   model for this. But it's critical that we only squash linear sequences and
   don't remove branching from the network, otherwise we'll lose recombination
   down the line.

   There's utility in keeping the history of every keystroke in your editor.
   That utility becomes a liability when publishing. The published history of
   your work should be a sample from the tree of edits you've made. Meta data
   might need to change, but the structure will have to be preserved.

   You ought to keep your superset of your published work for posterity (or
   yourself). It's surprising what you can learn about your own thought process
   by watching your keystrokes played back to you in real time.

   Archives and anthologies can in turn be coarser samplings of published works.
   Again the branching structure needs to be preserved, but details can be
   thrown away where they're not of interest.

   Mathematically these nested compressions form a fractal. Fractals make for
   natural compression algorithms, and the readers of your work will only ever
   need to download a tiny fraction of changes you've made to a blank page over
   your writing career.

   If it isn't obvious, my thinking was skewed by Ted Nelson's work decades ago.
   I don't have direct attributions at hand, but there's more than a little
   Xanadu in the above.

   That doesn't seem like the concern of a language per se. But when you
   consider that telemetry and debugging are mostly about rebuilding context
   that was thrown away for performance reasons, you'll see that it makes sense
   to think about it at the language runtime level as well.

   This is very high level. I'll add the details of how it works in this
   language (environment?) as the prototypes evolve.
** Transduction
** Message Passing
** JIT and runtime optimisation
   The AST itself is optimisable, large parts can be evaluated immediately, and
   it's fairly fast to interpret at runtime.

   I also want a native compiler. The up side of strict immutability, reentrant
   code, and static reference is that we never have to compile a form more than
   once. The compiled code will always be the same if we compile it the same
   way.

   But that said, we do want to try and compile things in different ways
   depending on profiling during real usage and compare performance (say compile
   both to cpu and gpu and instrument to figure out how much data makes the
   switch worthwhile). But that's just a cache of code plus compiler options.

   My intuition tells me that if we get the static reference right and most code
   never changes (only change code if there's a security vulnerability, or a
   massive and much needed performance improvement) then over time more and more
   code will be compiled in more and more ways so that very little will be
   interpreted and most of the runtime's work will be in applying heuristics to
   choose which compiled code to run on which occasion.

   Hardly a trivial problem, but there's real opportunity here.
** Work stealing
   The work stealing implementation is based on Cilk, but immutability
   simplifies a lot of things.

   When a μ emits messages, they are pushed into the work stack and control
   returns to the main loop. The main loop takes the top item off the stack and
   runs it. If the work stack is empty, then it attempts to steal work from
   another worker.

   The thief randomly shuffles the list of other known workers and tries to
   steal work from each of them until successful.

   When work is stolen, it is stolen from the *bottom* of the (upwards growing)
   stack. This is based on the reasoning that things at the bottom of the stack
   are closer to the root of the computation, will represent more work, and thus
   will minimise steals.

   Unlike cilk, we only need one stack. We don't need to worry about workers
   stepping on each other's caches because everything is immutable, so you can't
   invalidate the cache. Once work is stolen, it and its decendents will be
   executed by the thief unless they are stolen in turn.

   This should help with distribution, but stealing will need to be a bit more
   sophisticated to minimise data transfer over the network.

   One other note: as currently implemented, evaluating a vector of length N
   results in N messages being sent to eval, which can lead to each element
   being evaluated in parallel. Thus we get the auto parallelism of Multilisp
   — where (f x y z) will eval x, y, and z in parallel before applying them to f
   — without the complexity that comes with leaving it up to the programmer to
   ensure order of evaluation doesn't matter.

   We can also get pipeline parallelism automatically in long transduction
   chains, but that's a story for somewhere else.
** Offloading work to GPU
   Most consumer machines have gpus nowadays and a lot of streaming tasks map
   natuarally to them — though many others don't.

   Figuring out which can only be done in general at runtime.

   The plan is to use Vulkan instead of compute centred apis as is normally
   done, because there are vulkan drivers for basically every graphics chip out
   there. Plus you have a lot more flexibility than with cuda or opencl.

   Spirv takes a lot of inspiration from llvm, so code generation doesn't
   require going up to C and back down again as it does with glsl. This is only
   a theoretical benefit and I reserve the right to revile my present self at
   some future date.

   Plus there are a lot of cool things you can do with mesh shaders and ray
   tracers. They aren't just for graphics any more than pixel shaders are. Maybe
   a little bit more.
** Compiler
** FFI / Distributed Execution
   These aren't normally the same thing, but they both boil down to talking to
   other computers, so their solutions largely overlap.
* Comparisons
  I don't know how useful these are. Mostly thinking out loud.
** FP
   Immutability? Check. Big check.

   Types? Yes for the machine, no for the programmer. I'm thinking of putting in
   a clos/julia style polymorphism mechanism, but then I'd have to start
   worrying a lot more about types. I'm torn.

   Types in currently trendy languages seem to be all about being more static
   and preferring syntax over semantics. Types are about *shape*, not *meaning*
   and as such they're the same as text in a source file and in the runtime.
   That's a very powerful thing, but I have a generalisation of homoiconicity
   which strikes me as a lot more powerful. And more fun to use.

   Functions? Not really. Mathematical functions can be implemented easily, but
   they're a special case of a more general construct.

   The basic operator of the language, μ, takes one argument (receives one
   message) and sends zero or more messages on each of zero or more channels.
   This is based on Hickey's transduction and on Bird's work on Squiggol, but
   with with convergence and divergence of streams which makes it more powerful
   and more complicated. I still have an algebra though.
** FBP
   I had never heard of FBP until quite recently. Reading Morrison's book (2010)
   makes it clear that a lot of my ideas are not new and have been in use —
   albeit obscurely — for decades.

   Particularly the delivery guarantees of which I was pretty proud. But at
   least I know now that what seemed obviously sufficient has been battle tested
   and does seem to suffice.

   These aren't bad things. I'm not terribly interested in priority. I'm
   interested in making this work and the validation is appreciated.

   That said, there are some important differences between my goals and
   Morrison's description. A lot of those differences come from ideas that are
   relatively new. The idea that immutability can work at scale is not widely
   accepted even today. I doubt it was even conceivable thirty years ago.

   But let's list them out:
*** memory management
    In FBP they take an ownership approach where datagrams must be manually
    allocated and freed. This stems from the fact that each node in the network
    bangs on the memory passed to it.

    I take a different approach in that all messages are immutable. They don't
    need manual management since reference counting suffices. They also don't
    need to be owned, so one unit can send the same message to a dozen others.
    There are other advantages as well.

    The cost of immutability amortises away with isolated mutability.
*** Graphicality
    I'm a big fan of graphical programming, but I don't think it should be
    primary. The primary needs to be machine readable. Machine readability is
    the only way to get widespread interoperability. Short of Lanier's "use ai
    to click buttons as an API" which is very intriguing, but I'm still dubious.

    I contend that being more machine friendly at a low level allows you to be
    more human friendly at a high level. This is mostly because the majority of
    programming language bullshit is about making it easier for the developers
    of the compiler/editor/whatever, and if their job is already easy, they have
    fewer excuses.

    So instead I'm focusing on a traditional language with a (mostly)
    traditional AST, interpreter, and compiler. From that AST we can define
    isomorphisms with both the textual source code (or even multiple textual
    source codes) and a graphical representation.

    So draw your program at a high level and then go in later and tweak the
    code. Or vice versa. Or just stick to whichever you prefer.

    Graphical languages always have a layer of warts between the rubber and the
    road and I don't want that. or is it like studded tires?
*** Fractal Structure
    FBP has an explicitly fractal structure, so if you see a graph of
    components, you can zoom in on any piece and see a nested graph of
    components. This can go on for an indefinite period of time before finally
    bottoming out on something atomic.

    I think there's something fundamentally right about that structure.

    I'm not sure how to encourage it in the current iteration of this language.

    Any set of functions corresponds to what fbp would call the input ports of a
    network. The rest of the network is implicit in the call graph, but that's
    easy enough to follow along and layout on demand.

    So a network could be represented by a map of named functions.

    One major problem is that channels (ports) are not static. They can be
    dynamically generated. It can be, in general, impossible to determine the
    set of possible outputs of such a network statically since it may generate
    functions at runtime and plug them into the network. Technically those
    dynamic plugins would be subnetworks, but I don't know that that makes a
    difference.

    That dynamism isn't something I'm considering stripping from the language.
    That said, most of the time emissions go to named channels and the names are
    easily inferred from the code. There's also the secondary check that
    messages can only be sent on channels in the current context or newly
    generated ones, which gives us some foreknowledge. Likely not enough.
*** Holarchy
    The idea that a network is a whole unto itself in the sense that given
    inputs it does its thing and can't be intereferred with, but simultaneously
    subordinate in the sense that it doesn't control — and doesn't know —
    whence comes its input or whither goes its output, so that its place in the
    larger program is ordained from above, is implicit in what I've read of FBP,
    but never stated as such. Just taken for granted.

    I think — but don't know — that making this explicit by encouraging every
    subnetwork to believe itself to be the entire system will greatly
    facilitate the composition of systems.

    Somewhat hyperbolically, I want a language whose todomvc is kubernetes.

* References
  Some reading that ~might~ help you understand this project:
  - The Act of Creation — Arthur Koestler
  - Steps to an Ecology of Mind — Gregory Bateson
  - Seven Clues to the Origin of Life — A.G. Cairns-Smith
  - Do You Want to Enter the Area? — Charles Bukowski
    If you can't find that, try the posthumously watered down "So You Want to be
    a Writer?".
