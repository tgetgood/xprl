#+TITLE: A Primer

This is largely an aid to my own memory so bear with me.

* The Syntax
  I'll start by describing the syntax. Not because it's the most important nor
  the best place to start, but because it's the first thing you see.

  The syntax is, for the most part, inspired by Clojure. In particular, the
  literal data structures: =[]=, ={}=, & =#{}= mean what you expect.

  One major goal is that if an expression *looks* familiar, then it should act in
  a familiar way. The degree to which I've met that goal is an open question,
  but things are still in flux.

  #+BEGIN_SRC clojure
    (def f (fn [x y] (+ x y)))
    ;; => f

    (f 4 5)
    ;; => 9

    (if (< (f x y) 0) (g x) (h y))
  #+END_SRC

  and so on.
** Pairs vs Cons Cells
   I wanted to avoid cons cells entirely in this language, but I'm starting to
   realise why they're important: the symmetry between calling a function as (f
   x y z) and defining it as (fn [x y z] ...). The fact that the tail of the
   cons cell (f x y z) is the cons cell (x y z) makes this automatic. So... if
   I want to be able to call things with a single arg that isn't necessarily a
   list, I need improper cons cells. Once we have that, the convention of
   multiple args becomes automatic.

   That said, there are no cons *lists*. The linked list was once a great idea,
   but is, on modern hardware, just about the worst possible way to store
   data.

   Lisp uses cons lists in two ways: 1) as lists of data, for which we'll just
   use vectors, and 2) as a syntax for invoking functions.

   (f x y z) means eval the head (f), eval the tail (x y z), then apply the
   latter to the former.

   We will keep the (f x y z) syntax for calling functions, but instead of
   (cons f (cons x (cons y (cons z nil)))), we store it as (pair f [x y z]).

   This (f x y z) is identical to (f . [x y z]).

   The =.= can be interpreted in a functional sense as =apply=, or in an actor
   sense as =send=. It turns out they're both equivalent with the right
   underlying abstractions.
** No Implicit Evaluation
   In constrast to most lisps (in fact all I've ever found save one called
   [[https://web.cs.wpi.edu/~jshutt/kernel.html][Kernel]]), we have no implicit evaluation. That means that both arguments and
   return values are passed by name. That sounds crazy. If not for the Kernel
   thesis I'd never have believed such a scheme could work, but it does!

   To explain, I'll need to jump ahead to μ.
** The μ Operator
   Why μ? Two reasons.

   1) it's the next letter after λ.
   2) It looks weird, and I want things that don't act as expected to look
      abnormal.

   μ defines pure syntax operations.

   Example:

   #+BEGIN_SRC clojure
     (def f (μ [x y] (+ x y)))
     ;; => f

     (f 4 5)
     ;; => (+ x y)

     ;; which is an error because x and y aren't defined.
   #+END_SRC

   μ doesn't define a macro. A μ is first class: they can be passed and returned
   as values, they operate at runtime (or compile time, or anytime).

   But a μ has no direct access to the environment (nor the continuations). You
   can pass a namespace to a μ (it's just a map) and take it apart so as to
   perform any reflective feat your heart may desire, but the lexical environment
   of an expression is immutable and the expression must be redefined to change
   it, so there's nothing that can be done by the body of a μ with its own
   lexical environment.

   Also remember that macros evaluate their own output (or properly speaking the
   compiler expands the macro and then calls =eval= on the result), whereas μs do
   not.

   But if nothing evaluates the output of a μ, how do they *do* anything?
** Immediate Evaluation
   The answer to the hanging question of the last section is that there is a
   syntactic expression that "means" =eval=.

   In reality it means "evaluate the following expression as soon as you have
   all of the information required to do so", which will remain ambiguous until
   we get into the details of the AST.

   That syntax is the tilde =~=.

   #+BEGIN_SRC clojure
     (def f (μ [x y] (+ ~x ~y)))
     ;; => f

     (f 4 5)
     ;; => (+ 4 5)

     ;; `~x` and `~y` can be evaluated as soon as `f` is called. Since they're
     ;; bound in the lexical environment of the body, eval reduces to lookup.
     ;; But remember that the output is not evaluated, so `f` returns an expression.

     (def g (μ [x y] ~(+ x y)))
     ;; => g

     (g 4 5)
     ;; => 9

     ;; Note that this only works because `+` is defined so as to explicitly eval
     ;; its arguments before applying the underlying addition operation.
   #+END_SRC

   Given a function that only works on literal values (say a version of =+=
   called =+*=), we can define an operator =wrap= which will invoke it in the
   standard applicative manner by first evaluating its arguments:

   #+BEGIN_SRC clojure
     (def wrap
       (μ f
          ~(μ args
              ~(~f . ~~args))))

     (def + (wrap . +*))
   #+END_SRC

   We can even define λ (here called =fn=) as a μ which receives arguments,
   evaluates them, passes the evaluated values to another μ and then evaluates
   its output:

   #+BEGIN_SRC clojure
     (def fn
       (μ [params body]
          ~(μ args
              ~((μ ~params ~~body) . ~~args))))
   #+END_SRC

   This is the actual definition of =fn= at the core of the language.

   It may help to think of the body of a μ as quasiquoted, except that we can
   unquote as many times as we please.

   That said, there is no quote, unquote, quasiquote, eval, nor apply defined in
   the language — you can easily write them yourself, but please don't — so the
   analogy is a bit weak.
** An Exception
   One final note: I said above that there is *no* implicit evaluation. You've
   probably noticed in my examples that everything typed into the repl *is*
   automatically evaluated. The same holds for the code reader =loadfile=. This
   is a convenience that once again makes code that acts as expected look
   normal. Also there's no point entering code into a repl unless you want it to
   be evaluated, so it seems like the right thing to do.

   μs are, of course, free to evaluate their arguments when and as they see fit.
   =def= adds an evaluation to the beginning of its body, for instance. I think
   that's the right thing to do as well, but it adds a bit of asymmetry to the
   definition of metaprogramming operations like =fn= and =wrap=.
* Reduction and the AST
  As covered in the previous section, whether or not an expression will ever be
  evaluated — that is whether a given expression is code or data — is expressed
  syntactically. Given =(f x)= the question of whether =x= will or will not be
  evaluated doesn't depend on the *type* of =f=, but on its syntactic
  representation.

  Syntax is topological. But it has no notion of time. The question of *whether*
  an expression will be evaluated is syntactic, but the question of *when* an
  expression will be evaluated is contextual: it depends on the unknowns in the
  expression and on whoever provides them.

  Take for example, the humble =λ=. Copying from above, we have:

   #+BEGIN_SRC clojure
   (def fn
     (μ [params body]
        ~(μ args
            ~((μ ~params ~~body) . ~~args))))
   #+END_SRC

   Notice that everything here is a symbol, even =def= and =μ=. This expression
   can only be evaluated in an environment that provides meaning for those
   symbols. And yes, you can override them if you want to. You can also rebind
   =self-insert-command= if you feel like it.

   Some notes on abstract syntax: =(f x y)= is a Pair, in our parlance. The head
   is the ~symbol~ =f= and the tail is the list (of symbols) =[x y]=.

   A Pair is just data. It doesn't represent a function application. An
   Application is represented by =#(...)= to distinguish it from a pair.

   =#(f x y)= represents the application of =[x y]= to the *symbol* =f=, which
   is senseless and would result in error.

   In a traditional lisp, we have
   =(eval (f . tail))= => =(apply (eval f) (eval tail))=.

   But we aren't writing an applicative language, so we want something more
   like:

   =(eval (f . tail))= => =(apply (eval f) tail)=

   Which is represented syntactically by our first rule:

   =~(f . tail)= => =#(~f . tail)=

   Thus ~(f x y) -> #(~f x y) which applies [x y] to whatever the symbol =f= is
   bound to. The μ to which f is bound, might in turn choose to evaluate
   (lookup) x and y to do something with them.

   The second thing we need to do is create actual μ objects. I don't believe
   its possible to implement μ in this language, so we need an escape hatch
   which I've taken to calling primitive macros.

   Let =μ= be bound to the primitive =#Pmac[createμ]= then we have

   =~(μ params body)= => =#(#Pmac[createμ] params body)=

   which causes the underlying implementation of the primitive =createμ= to be
   called with the tail of the application: =[params body]= as well as the
   current environment and continuation map.

   It's up to a primitive macro to continue appropriately, which makes them very
   brittle. As of yet I only need 4 of them. They're all related to manipulating
   context. Notably all control flow can be implemented as normal μs in the
   language itself.

   There are also primitive functions which can only be applied to literal
   values. These aren't essential to having a working language, but while you
   could implement arithmetic lambda calculus style, it's advisable to wrap the
   machine's arithmetic capabilities (or those of the implementation language at
   least).

   Primitive functions are best thought of as functions implemented in another
   language. We'll come back to them later.

   Our third rule of execution involves what it means to apply arguments to a μ.

   If we let =(#μ params body)= represent an actual μ object with =params= and
   =body=, then we can write:

   =#((#μ params body) . args)= =>
   =(extend-enviroment (destructuring-bind params args) body)=

   Note that =extend-environment= and =destructuring-bind= are only defined in
   the implementation at present. Bootstrapping is not a short term goal.

   This is just Steele's "λ the ultimate declarative" without applicative
   semantics. A μ takes arguments, binds them to formal parameters, and then
   reduces its body in the new, extended environment.

   N.B.: "reduction" in this context refers to applying the rules we're laying
   down. They simplify an expression without necessarily evaluating it. If
   there's sufficient information, then reduction will fully interpret an
   expression, but in general there will be arguments that aren't known yet.

   There's a special case of applying arguments which is just β-reduction:

   =#((#μ X Y) . ~Z)= (where X, Y, & Z stand for subexpressions)

   becomes Y with symbols occuring in X renamed to their bindings in Z.

   In particular =#((#μ X Y) . ~X)= => =Y=. This actually comes up a bunch when
   compiling, so it's important to recognise.

   Finally, eval distributes over lists, and literal values are fixed points to
   eval.

   That is: =~V= => =V= if V is a literal value (number, string, bool, etc.)

   and: =~[X Y Z]= => =[~X ~Y ~Z]=
** Summary
   Reduction is the process of applying the following rules repeatedly until
   none can be applied any longer:

   An expression is considered "fully reduced" if it contains no immediate or
   application nodes.

   1) ~(X . Y) <=> #(~X . Y)
   2) #(#Pmac[f] . tail) => always escapes to implementation
   3) #(#Pfn[f] . tail) => invokes foreign code iff tail is fully reduced.
   4) #((#μ X Y) . Z) => Y in the current lexical env extended by {X Z}
      where {X Z} represents the destructured binding of X to Z

      The essentially says the ~X inside Y means Z.
   5) #((#μ X Y) . ~Z) => Y with {X Z} as a renaming mask
      If ~X means ~Z then use of X stands for use of Z.

      The special case Z=X makes this a noop.
   6) ~V <=> V whenever V is a literal (scalar) value
   7) ~[X Y ...] <=> [~X ~Y ...]
      Similar rules apply to maps and sets in the obvious fashion.

   Some of these rules are reversable, some are not because information is lost.

   The order in which these rules are applied is irrelevant since all
   permutations will result in the same final state (possibly with some
   backtracking over the reversible rules, I'm not sure).

   I have a handwavy proof that validates my intuition regarding the previous
   assertion, so be skeptical.

   This (indirectly) answers the question of when an immediate expression gets
   evaluated. By following the rules from a valid input, you will always end up
   at a value. That value might be a μ waiting on arguments. Obviously the
   interior of that μ will be blocked to some extent until those arguments are
   known.

   Note on implementation:

   The current interpreter implicitly embodies these rules. When I get around to
   writing a compiler, the first step will be an explicit search over these
   rules to find a "maximally reduced normal form" which can then be translated
   into lower level code. But that's for another day.
* The Environment
* The Continuation Map and Emission
  One of the bigger differences between this language and standard lisp (or any
  von neumann style language with which I'm familiar) is that there is no single
  "current continuation", but instead a set of them. And control can pass to any
  number of them from the current expression. It can even pass to several of
  them several times each.

  This is not too different from an actor system, but messages are always sent
  in batches from tail position. The continuation map is akin to Erlang's
  "address book", but it's aggressively local and indirected. You can't send
  messages to named things, you send messages on named channels (whose name you
  choose), and something above you in the call graph binds those names to μs.

  You can also send messages to functions directly, but since functions are
  pure, this isn't like sending messages to actors, it's more like setting up a
  subcomputation.

  The underlying theory comes from what Koestler called holarchy which is the
  idea that each unit is autonomous from its own point of view: it can set up
  subcomputations and connect the results of those to channels, but it's
  simultaneously subordinate to higher levels in that it doesn't ultimately know
  how its outputs will be used, nor whence its inputs come.

  A more programming friendly metaphor: Pure functions can't do anything at the
  end of the day. That's why we needed monads before we could create Haskell.

  Do blocks (progn for the die hard lispers) exist because we generally want our
  code to do things other than compute and return answers. These side effects
  are the source of all kinds of complexity. We can't live without them, but oh
  boy.

  So, I've made a change to the traditional structure: there are no progns.
  Function arguments are not evaluated in order. You can't control the sequence
  in which things happen at all. All you can do is establish data dependencies
  between computations and then send batches of messages.

  So if you want to add logging to a program, you can't just stick in a
  statement and have a side effect coming from nowhere. But you can intercept a
  batch of messages and add one more message to be sent to a logging queue.

  So we can still perform side effects, but those side effects must exist as
  pure data (messages) sent to the things that actually bang on the world. Those
  messages can be logged, replayed, or even filtered. That makes a big
  difference.

  The continuation map doesn't exist as a first class thing from within the
  language. Rather it is maintained by the runtime (interpreter presently) and
  can be modified by calling primitive macros. I don't want to write code in cps
  if I can avoid it.

  When a batch of messages are emitted, they are all delivered (logically
  speaking) concurrently. If there's enough spare compute, they'll be delivered
  (execute) in parallel, otherwise they get queued. See the section on work
  stealing for details. Not being able to depend on order of delivery saves us a
  world of pain when it comes to starvation and race conditions.

  The only guarantees we make on delivery are

  1) messages are delivered strictly after they are sent
  2) multiple messages sent from A to B are delivered exactly once and in order

  Note that if A and C both send messages to B, then those streams can be
  interleaved in any fashion, but the subsets from A and C will be in the order
  sent.

  These are the only guarantees we can provide without pretending special
  relativity doesn't apply to us. For better or for worse, it does apply to us.

  In essence, a fully consistent totally ordered view of the world is only
  possible from a single point in space. Once you have multiple actors in
  different locations, perspective becomes an irreducible aspect of the system.

  So if you want a consistent view, put messages in a queue and process them in
  serial. It's not that hard.

  Now it's time to talk about our next special construct. The Stateful.
* Statefuls
  The name needs some work.

  Sometimes a computation needs to retain state across messages. Given that we
  have no mechanism by which to assign or mutate, how do we accomplish it?

  The easiest way would be to have a μ which from tail position sends a messages
  to its future self (plus whatever emissions it needs to make).

  This would be a perfect solution except for the fact that we have no control
  over the order of delivery, so how do we know that your future self will
  receive its new state before it receives its next input?

  Sadly you can't.

  Thus, as far as I can tell — and I may be wrong, I hope I'm wrong, but I don't
  think I'm wrong — we need a special construct to represent a stateful
  accumulator.

* Outline
  Sections to be written.
** History, Environment, and Context
   I gave a [[https://www.youtube.com/watch?v=Kgw9fblSOx4][talk]] on some of these ideas a long time ago. My opinions on the
   benefits haven't changed, but I've found better ways to implement them.

   You can only have unique permalinks without global concensus if:

   1) You and the link author share some context
      At the very least you have to look in the right place.
   2) The links can grow in size arbitrarily
      Fixed size hashes will eventually lead to collision. I don't care how few
      atoms there are in the observable universe.
   3) Divergences can be converged without consulting either author
      If two people who've never heard of each other publish under the same hash
      at roughly the same time, anyone who knows of both must be able to
      "decorrupt" the hashmap by themselves in a way that anyone else with whom
      they communicate can understand and replicate. The original authors might
      be lightyears away so they cannot be relied upon. Even if they're right
      here, they can't be relied upon to agree.

   None of those criteria have anything to do with hashes.

   Hashes are useful for their concision and cease to be beneficial when you
   start to need sha512 to guarantee uniqueness.

   Using large hashes makes microreferences infeasible for the same reason that
   credit card fees make microtransactions infeasible.

   It turns out that if you keep a history of edits going all the way back to
   the beginning of the web, then you and the person you're sending to (or
   receiving from) can always go back to a shared context and regrow from there.

   In general you don't even need to go back very far if you communicate with
   this other person frequently.

   So my approach is to never throw away history. Spool it out to disk or other
   durable storage if you don't need it in ram (and you don't, mostly) but don't
   throw it away.

   Some sort of compression will required eventually. Kafka has a decent model
   for this. But it's critical that we only squash linear sequences and don't
   remove branching from the network, otherwise we'll lose recombination down
   the line.

   There's utility in keeping the history of every keystroke in your editor.
   That utility becomes a liability when publishing. The published history of
   your work should be a sample from the tree of edits you've made. Meta data
   might need to change, but the structure will have to be preserved.

   You ought to keep your superset of your published work for posterity (or
   yourself). It's surprising what you can learn about your own thought process
   by watching your keystrokes played back to you in real time.

   Archives and anthologies can in turn be coarser samplings of published works.
   Again the branching structure needs to be preserved, but details can be
   thrown away where they're not of interest.

   Mathematically these nested compressions form a fractal. Fractals make for
   natural compression algorithms, and the readers of your work will only ever
   need to download a tiny fraction of changes you've made to a blank page over
   your writing carreer.

   If it isn't obvious, my thinking was skewed by Ted Nelson's work decades ago.
   I don't have direct attributions at hand, but there's more than a little
   Xanadu in the above.

   That doesn't seem like the concern of a language per se. But when you
   consider that telemetry and debugging are mostly about rebuilding context
   that was thrown away for performance reasons, you'll see that it makes sense
   to think about it at the language runtime level as well.

   This is very high level. I'll add the details of how it works in this
   language (environment?) as the prototypes evolve.
** Transduction
** Message Passing
** JIT and runtime optimisation
   The AST itself is optimisable, large parts can be evaluated immediately, and
   it's fairly fast to interpret at runtime.

   I also want a native compiler. The up side of strict immutability, reentrant
   code, and static reference is that we never have to compile a form more than
   once. The compiled code will always be the same if we compile it the same
   way.

   But that said, we do want to try and compile things in different ways
   depending on profiling during real usage and compare performance (say compile
   both to cpu and gpu and instrument to figure out how much data makes the
   switch worthwhile). But that's just a cache of code plus compiler options.

   My intuition tells me that if we get the static reference right and most code
   never changes (only change code if there's a security vulnerability, or a
   massive and much needed performance improvement) then over time more and more
   code will be compiled in more and more ways so that very little will be
   interpreted and most of the runtime's work will be in applying heuristics to
   choose which compiled code to run on which occasion.

   Hardly a trivial problem, but there's real opportunity here.
** Work stealing
   The work stealing implementation is based on Cilk, but immutability
   simplifies a lot of things.

   When a μ emits messages, they are pushed into the work stack and control
   returns to the main loop. The main loop takes the top item off the stack and
   runs it. If the work stack is empty, then it attempts to steal work from
   another worker.

   The thief randomly shuffles the list of other known workers and tries to
   steal work from each of them until successful.

   When work is stolen, it is stolen from the *bottom* of the (upwards growing)
   stack. This is based on the reasoning that things at the bottom of the stack
   are closer to the root of the computation, will represent more work, and thus
   will minimise steals.

   Unlike cilk, we only need one stack. We don't need to worry about workers
   stepping on each other's caches because everything is immutable, so you can't
   invalidate the cache. Once work is stolen, it and its decendents will be
   executed by the thief unless they are stolen in turn.

   This should help with distribution, but stealing will need to be a bit more
   sophisticated to minimise data transfer over the network.

   One other note: as currently implemented, evaluating a vector of length N
   results in N messages being sent to eval, which can lead to each element
   being evaluated in parallel. Thus we get the auto parallelism of Multilisp
   — where (f x y z) will eval x, y, and z in parallel before applying them to f
   — without the complexity that comes with leaving it up to the programmer to
   ensure order of evaluation doesn't matter.

   We can also get pipeline parallelism automatically in long transduction
   chains, but that's a story for somewhere else.
** Offloading work to GPU
   Most consumer machines have gpus nowadays and a lot of streaming tasks map
   natuarally to them — though many others don't.

   Figuring out which can only be done in general at runtime.

   The plan is to use Vulkan instead of compute centred apis as is normally
   done, because there are vulkan drivers for basically every graphics chip out
   there. Plus you have a lot more flexibility than with cuda or opencl.

   Spirv takes a lot of inspiration from llvm, so code generation doesn't
   require going up to C and back down again as it does with glsl. This is only
   a theoretical benefit and I reserve the right to revile my present self at
   some future date.

   Plus there are a lot of cool things you can do with mesh shaders and ray
   tracers. They aren't just for graphics any more than rasterisers are. Maybe a
   little bit more.
** Compiler
** FFI / Distributed Execution
   These aren't normally the same thing, but they both boil down to talking to
   other computers, so their solutions largely overlap.
* Comparisons
  I don't know how useful these are. Mostly thinking out loud.
** FP
   Immutability? Check. Big check.

   Types? Yes for the machine, no for the programmer. I'm thinking of putting in
   a clos/julia style polymorphism mechanism, but then I'd have to start
   worrying a lot more about types. I'm torn.

   Types in currently trendy languages seem to be all about being more static
   and preferring syntax over semantics. Types are about *shape*, not *meaning*
   and as such they're the same as text in a source file and in the runtime.
   That's a very powerful thing, but I have a generalisation of homoiconicity
   which strikes me as a lot more powerful. And more fun to use.

   Functions? Not really. Mathematical functions can be implemented easily, but
   they're a special case of a more general construct.

   The basic operator of the language, μ, takes one argument (receives one
   message) and sends zero or more messages on each of zero or more channels.
   This is based on Hickey's transduction and on Bird's work on Squiggol, but
   with with convergence and divergence of streams which makes it more powerful
   and more complicated. I still have an algebra though.
** FBP
   I had never heard of FBP until quite recently. Reading Morrison's book (2010)
   makes it clear that a lot of my ideas are not new and have been in use —
   albeit obscurely — for decades.

   Particularly the delivery guarantees of which I was pretty proud. But at
   least I know now that what seemed obviously sufficient has been battle tested
   and does seem to suffice.

   These aren't bad things. I'm not terribly interested in priority. I'm
   interested in making this work and the validation is appreciated.

   That said, there are some important differences between my goals and
   Morrison's description. A lot of those differences come from ideas that are
   relatively new. The idea that immutability can work at scale is not widely
   accepted even today. I doubt it was even conceivable thirty years ago.

   But let's list them out:
*** memory management
    In FBP they take an ownership approach where datagrams must be manually
    allocated and freed. This stems from the fact that each node in the network
    bangs on the memory passed to it.

    I take a different approach in that all messages are immutable. They don't
    need manual management since reference counting suffices. They also don't
    need to be owned, so one unit can send the same message to a dozen others.
    There are other advantages as well.

    The cost of immutability amortises away with isolated mutability.
*** Graphicality
    I'm a big fan of graphical programming, but I don't think it should be
    primary. The primary needs to be machine readable. Machine readability is
    the only way to get widespread interoperability. Short of Lanier's "use ai
    to click buttons as an API" which is very intriguing, but I'm still dubious.

    I contend that being more machine friendly at a low level allows you to be
    more human friendly at a high level. This is mostly because the majority of
    programming language bullshit is about making it easier to for the
    developers of the compiler/editor/whatever, and if their job is already
    easy, they have fewer excuses.

    So instead I'm focusing on a traditional language with a (mostly)
    traditional AST, interpreter, and compiler. From that AST we can define
    isomorphisms with both the textual source code (or even multiple textual
    source codes) and a graphical representation.

    So draw your program at a high level and then go in later and tweak the
    code. Or vice versa. Or just stick to whichever you prefer.

    Graphical languages always have a layer of warts between the rubber and the
    road and I don't want that.
*** Fractal Structure
    FBP has an explicitly fractal structure, so if you see a graph of
    components, you can zoom in on any piece and see a nested graph of
    components. This can go on for an indefinite period of time before finally
    bottoming out on something atomic.

    I think there's something fundamentally right about that structure.

    I'm not sure how to encourage it in the current iteration of this language.

    Any set of functions corresponds to fbp would call the input ports of a
    network. The rest of the network is implicit in the call graph, but that's
    easy enough to follow along and layout on demand.

    So a network could be represented by a map of named functions.

    One major problem is that channels (ports) are not static. They can be
    dynamically generated. It can be, in general, impossible to determine the
    set of possible outputs of such a network statically since it may generate
    functions at runtime and plug them into the network. Technically those
    dynamic plugins would be subnetworks, but I don't know that that makes a
    difference.

    That dynamism isn't something I'm considering stripping from the language.
    That said, most of the time emissions go to named channels and the names are
    easily inferred from the code. There's also the secondary check that
    messages can only be sent on channels in the current context or newly
    generated ones, which gives us some foreknowledge. Likely not enough.
*** Holarchy
    The idea that a network is a whole unto itself in the sense that given
    inputs it does its thing and can't be intereferred with, but simultaneously
    subordinate in the sense that it doesn't control — and doesn't know —
    whither comes its input or whence goes its output, so that its place in the
    larger program is ordained from above, is implicit in what I've read of FBP,
    but never stated as such. Just taken for granted.

    I think — but don't know — that making this explicit by encouraging every
    subnetwork to believe itself to be the entire system will greatly
    fascilitate the composition of systems.

    Somewhat hyperbolically, I want a language whose todomvc is kubernetes.

* References
  Some reading that ~might~ help you understand this project:
  - The Act of Creation — Arthur Koestler
  - Steps to an Ecology of Mind — Gregory Bateson
  - Seven Clues to the Origin of Life — A.G. Cairns-Smith
  - Do You Want to Enter the Area? — Charles Bukowski
    If you can't find that, try the posthumously watered down "So You Want to be
    a Writer?".
