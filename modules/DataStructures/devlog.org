* Optimisation log
  Initial performance was unsuitable for anything but a toy. This (poorly)
  tracks my attempts to make it practical.
** notes [2023-09-18 Mon]
   For normal use these datastructures seem to perform adequately.

   However, when trying to load textures with the following code:

   #+BEGIN_SRC julia
     @time rgb = ds.into(
       ds.emptyvector,
       map(p -> [
         reinterpret(UInt8, p.b), reinterpret(UInt8, p.g), reinterpret(UInt8, p.r),
         0xff
       ])
       ∘
       ds.cat(),
       ds.take(x, image)
     )
   #+END_SRC

   Where =image= is an array of pixels, I get some fun results. For x = 2-2^15:

   2:   0.031788 seconds (46.67 k allocations: 9.054 MiB, 98.30% compilation time)
   4:   0.006815 seconds (84 allocations: 12.007 MiB, 85.72% gc time)
   8:   0.001803 seconds (164 allocations: 24.023 MiB)
   16:   0.021166 seconds (3.82 k allocations: 48.276 MiB, 23.66% gc time, 40.25% compilation time)
   32:   0.016689 seconds (866 allocations: 96.120 MiB, 41.37% gc time)
   64:   0.065319 seconds (48.74 k allocations: 195.433 MiB, 15.55% gc time, 58.27% compilation time)
   128:   0.044260 seconds (4.05 k allocations: 384.511 MiB, 38.96% gc time)
   256:   0.075904 seconds (8.30 k allocations: 769.056 MiB, 31.04% gc time)
   512:   0.143548 seconds (18.83 k allocations: 1.502 GiB, 32.47% gc time)
   1024:   0.288988 seconds (39.91 k allocations: 3.005 GiB, 32.32% gc time)
   2048:   0.570507 seconds (84.10 k allocations: 6.012 GiB, 32.30% gc time)
   4096:   1.138775 seconds (172.48 k allocations: 12.030 GiB, 32.34% gc time)
   8192:   2.278270 seconds (349.25 k allocations: 24.086 GiB, 32.43% gc time)
   16384:   4.563693 seconds (702.79 k allocations: 48.270 GiB, 32.32% gc time)
   32768:   9.203719 seconds (1.48 M allocations: 96.883 GiB, 32.35% gc time)

   Which extrapolated up to the full 1M pixels of the image in question, this would
   require 20M allocations and 3TB of allocated memory. Of course my machine runs
   out of ram long before that can be tested.

   The naive jl code:

   #+BEGIN_SRC julia
     rgb::Vector{UInt8}  = reduce(
       vcat,
       map(p -> [
         reinterpret(UInt8, p.b), reinterpret(UInt8, p.g), reinterpret(UInt8, p.r),
         0xff
       ],
           image
           )
     )
   #+END_SRC

   0.577784 seconds (2.15 M allocations: 2.243 GiB, 25.86% gc time, 5.11% compilation

   So the number of allocations in the persistent case is is ~10x higher and the
   memory allocated and collected is >1000x higher.

   10x on both counts would actually be acceptable for my exploratory use case, but
   the 1000-1500x doesn't fly.

   Curiously, if I use Base.Vector, it takes longer and uses more ram. It also seg
   faults at 2^15 which I think is actually a stack overflow.

   The idea was to use Base.Vector and Base.reduce and have better performace. That
   needs a redesign.
** improvements [2023-09-28 Thu]

   After reimplementing vectors using Tuples instead of arrays (not to mention
   fixing up some blatant errors, we get: 2:   0.037318 seconds (34.10 k allocations: 2.270 MiB) 4:   0.000030 seconds (147 allocations: 3.234 KiB)
   8:   0.000036 seconds (299 allocations: 6.750 KiB)
   16:   0.012085 seconds (4.31 k allocations: 261.349 KiB)
   32:   0.021579 seconds (7.78 k allocations: 456.428 KiB)
   64:   0.040643 seconds (14.73 k allocations: 852.820 KiB)
   128:   0.079438 seconds (28.61 k allocations: 1.631 MiB)
   256:   0.154938 seconds (56.38 k allocations: 3.331 MiB)
   512:   0.005223 seconds (42.55 k allocations: 1.547 MiB)
   1024:   0.014135 seconds (102.44 k allocations: 4.144 MiB)
   2048:   0.049300 seconds (245.93 k allocations: 13.219 MiB, 25.63% gc time)
   4096:   0.088340 seconds (565.65 k allocations: 33.909 MiB)
   8192:   0.250066 seconds (1.35 M allocations: 76.654 MiB, 5.44% gc time)
   16384:   0.580798 seconds (3.10 M allocations: 166.967 MiB, 4.01% gc time)
   32768:   1.288246 seconds (6.79 M allocations: 368.141 MiB, 3.48% gc time)
   65536:   2.873495 seconds (14.92 M allocations: 894.722 MiB, 3.85% gc time)
   131072:   6.365209 seconds (32.24 M allocations: 1.982 GiB, 4.17% gc time)
   262144:  14.536195 seconds (71.52 M allocations: 4.240 GiB, 3.77% gc time)
   524288:  32.252954 seconds (155.95 M allocations: 8.910 GiB, 3.66% gc time)
   1048576:  68.632605 seconds (330.91 M allocations: 18.891 GiB, 3.62% gc time)

   ~10x improvement in run time. Huge improvement in allocated space, but number of
   allocations is ~6x higher.

   Lesson 1) transients are indispensible. Lesson 2) maybe tuples weren't such a
   hot idea after all.

   I'll try out the streamlined design with vectors again. I might be thrashing
   here.

   Easy wins are parallel transduction and transients. Maybe I should focus my
   efforts there.

   I also massively simplified the implementation of maps, and added some tests to
   convince myself they work (to the extent I've thought to use them). But again
   huge performance regression. They're basically unusable as is.

   After reimplementing with (base) vectors instead of tuples, I get:
   2:   0.000036 seconds (68 allocations: 2.328 KiB)
   4:   0.000011 seconds (138 allocations: 4.891 KiB)
   8:   0.000017 seconds (278 allocations: 10.938 KiB)
   16:   0.000050 seconds (930 allocations: 37.641 KiB)
   32:   0.000094 seconds (2.35 k allocations: 95.234 KiB)
   64:   0.000207 seconds (5.57 k allocations: 225.328 KiB)
   128:   0.000493 seconds (13.54 k allocations: 619.172 KiB)
   256:   0.001366 seconds (36.66 k allocations: 1.819 MiB)
   512:   0.003244 seconds (88.02 k allocations: 4.183 MiB)
   1024:   0.015228 seconds (193.80 k allocations: 9.068 MiB, 59.99% gc time)
   2048:   0.012516 seconds (417.67 k allocations: 19.494 MiB)
   4096:   0.036217 seconds (914.55 k allocations: 45.283 MiB, 24.31% gc time)
   8192:   0.079682 seconds (2.10 M allocations: 113.657 MiB, 18.27% gc time)
   16384:   0.158852 seconds (4.74 M allocations: 245.702 MiB, 13.64% gc time)
   32768:   0.338404 seconds (10.12 M allocations: 514.794 MiB, 14.86% gc time)
   65536:   0.701287 seconds (21.26 M allocations: 1.049 GiB, 14.67% gc time)
   131072:   1.523102 seconds (45.11 M allocations: 2.295 GiB, 16.00% gc time)
   262144:   3.396177 seconds (99.11 M allocations: 5.313 GiB, 16.90% gc time)
   524288:   7.134940 seconds (215.47 M allocations: 11.204 GiB, 17.91% gc time)
   1048576:  14.954999 seconds (451.32 M allocations: 23.141 GiB, 19.62% gc time)

   With no runtime compiler warmup to speak of, which is an added bonus.

   so now we're down to 10x ram, 30x runtime, and 200x allocations. But that's
   without transients. Progress.

   And to think I went with tuples because I thought they would optimise better
   being immutable. And they do in terms of allocations and memory usage. But 1.28x
   more RAM for a 4.5x runtime boost seems like a good trade.

   [2023-09-29 Fri]

   2:   0.000040 seconds (72 allocations: 2.141 KiB)
   4:   0.000011 seconds (142 allocations: 4.453 KiB)
   8:   0.000018 seconds (282 allocations: 10.000 KiB)
   16:   0.000050 seconds (866 allocations: 32.734 KiB)
   32:   0.000095 seconds (2.14 k allocations: 81.078 KiB)
   64:   0.000218 seconds (5.07 k allocations: 189.672 KiB)
   128:   0.000470 seconds (12.47 k allocations: 528.500 KiB)
   256:   0.001161 seconds (33.42 k allocations: 1.561 MiB)
   512:   0.002527 seconds (77.35 k allocations: 3.539 MiB)
   1024:   0.005505 seconds (168.29 k allocations: 7.620 MiB)
   2048:   0.011057 seconds (362.45 k allocations: 16.345 MiB)
   4096:   0.032831 seconds (799.94 k allocations: 38.357 MiB, 28.70% gc time)
   8192:   0.071598 seconds (1.87 M allocations: 97.677 MiB, 23.86% gc time)
   16384:   0.130356 seconds (4.11 M allocations: 209.177 MiB, 12.99% gc time)
   32768:   0.281846 seconds (8.69 M allocations: 436.178 MiB, 16.36% gc time)
   65536:   0.582891 seconds (18.24 M allocations: 908.179 MiB, 16.01% gc time)
   131072:   1.223732 seconds (38.90 M allocations: 1.951 GiB, 16.16% gc time)
   262144:   2.918031 seconds (86.53 M allocations: 4.558 GiB, 17.62% gc time)
   524288:   6.131520 seconds (184.93 M allocations: 9.550 GiB, 18.30% gc time)
   1048576:  12.946524 seconds (384.87 M allocations: 19.658 GiB, 20.43% gc time)

   Just by overloading conj with a fast case when the element being added is a
   subtype of the collection type.

   I didn't expect reflection (typejoin) to be cheap, but that one call can't
   account for what's going on here. Looking at the LLVM bitcode, the generate code
   is superficially similar, but I'm not familiar enough to compare them without
   great effort which I'd rather allocate elsewhere.

   Maybe allowing the possibility of having to box at every step causes all the
   extra work. In any case I've learned to be much more wary of reflection.

   And removing the asserts gets us:

   2:   0.266746 seconds (696.97 k allocations: 46.322 MiB, 99.87% compilation time)
   4:   0.000036 seconds (142 allocations: 4.453 KiB)
   8:   0.000034 seconds (282 allocations: 10.000 KiB)
   16:   0.188647 seconds (130.75 k allocations: 8.612 MiB, 8.30% gc time, 99.93% compilation time)
   32:   0.000098 seconds (1.85 k allocations: 72.125 KiB)
   64:   0.000166 seconds (3.95 k allocations: 161.656 KiB)
   128:   0.000365 seconds (8.14 k allocations: 438.359 KiB)
   256:   0.000868 seconds (16.52 k allocations: 1.250 MiB)
   512:   0.008639 seconds (42.78 k allocations: 2.864 MiB, 77.68% compilation time)
   1024:   0.003965 seconds (94.47 k allocations: 6.089 MiB)
   2048:   0.008332 seconds (198.40 k allocations: 12.750 MiB)
   4096:   0.033216 seconds (406.25 k allocations: 29.133 MiB, 47.31% gc time)
   8192:   0.049455 seconds (821.97 k allocations: 70.945 MiB, 25.66% gc time)
   16384:   0.100161 seconds (1.98 M allocations: 153.680 MiB, 16.17% gc time)
   32768:   0.188255 seconds (4.29 M allocations: 320.151 MiB, 10.64% gc time)
   65536:   0.394972 seconds (8.92 M allocations: 659.092 MiB, 12.61% gc time)
   131072:   0.855680 seconds (18.18 M allocations: 1.401 GiB, 13.63% gc time)
   262144:   1.972930 seconds (36.69 M allocations: 3.199 GiB, 15.42% gc time)
   524288:   4.259396 seconds (84.20 M allocations: 6.769 GiB, 17.15% gc time)
   1048576:   8.968445 seconds (179.22 M allocations: 13.939 GiB, 18.54% gc time)

   Which really oughtn't be surprising...

   One idea which I ought to look into is preallocating vectorleaves in certain
   contexts. Creating vectors with Vector(undef, 32) and tracking indicies instead
   of using `end` actually caused a substantial regression in performance. I don't
   know why. It took more memory to allocate less? Each node had an extra byte to
   store the index, but that doesn't account for it.
** [2023-09-29 Fri] Transients

   first crack at transients and they do a lot more harm than good:

   2:   0.402445 seconds (670.46 k allocations: 44.595 MiB, 3.47% gc time, 99.94% compilation time)
   4:   0.000037 seconds (136 allocations: 3.953 KiB)
   8:   0.000051 seconds (260 allocations: 7.531 KiB)
   16:   0.137807 seconds (286.42 k allocations: 19.423 MiB, 99.81% compilation time)
   32:   0.127717 seconds (279.40 k allocations: 18.837 MiB, 99.75% compilation time)
   64:   0.000405 seconds (7.09 k allocations: 170.391 KiB)
   128:   0.001063 seconds (24.48 k allocations: 535.469 KiB)
   256:   0.003657 seconds (89.97 k allocations: 1.798 MiB)
   512:   0.013832 seconds (343.82 k allocations: 6.599 MiB)
   1024:   0.053068 seconds (1.34 M allocations: 25.202 MiB)
   2048:   0.241664 seconds (5.31 M allocations: 98.406 MiB, 13.25% gc time)
   4096:   0.915464 seconds (21.10 M allocations: 388.814 MiB, 4.25% gc time)
   8192:   3.544060 seconds (84.15 M allocations: 1.509 GiB, 3.20% gc time)
   16384:  14.464104 seconds (336.06 M allocations: 6.019 GiB, 3.89% gc time)
   abort...

   Locking is the first culprit to look into. Is there a way to enforce thread
   isolation at a higher level and not worry about locks?
** [2023-10-01 Sun] Transients Take Two
   2:   0.235361 seconds (433.40 k allocations: 28.939 MiB, 6.07% gc time, 99.92% compilation time)
   4:   0.000030 seconds (203 allocations: 5.906 KiB)
   8:   0.000031 seconds (387 allocations: 11.234 KiB)
   16:   0.189812 seconds (401.51 k allocations: 26.632 MiB, 99.90% compilation time)
   32:   0.115477 seconds (167.73 k allocations: 11.353 MiB, 12.64% gc time, 99.81% compilation time)
   64:   0.000239 seconds (3.54 k allocations: 105.094 KiB)
   128:   0.000391 seconds (7.17 k allocations: 213.625 KiB)
   256:   0.000780 seconds (14.44 k allocations: 430.000 KiB)
   512:   0.067743 seconds (205.24 k allocations: 12.381 MiB, 96.26% compilation time)
   1024:   0.007544 seconds (93.54 k allocations: 2.309 MiB)
   2048:   0.017824 seconds (208.90 k allocations: 5.000 MiB)
   4096:   0.038943 seconds (439.63 k allocations: 10.382 MiB)
   8192:   0.081237 seconds (901.09 k allocations: 21.144 MiB)
   16384:   1.162575 seconds (11.70 M allocations: 193.909 MiB, 4.27% gc time)
   32768:   4.961132 seconds (53.15 M allocations: 843.174 MiB, 1.58% gc time)
   65536:  12.987711 seconds (136.04 M allocations: 2.092 GiB, 1.90% gc time)
   fail...

   10x better than transients take I, but still 100x worse than straight up
   persistence...

   What the hell? I've got a lot to learn it would appear.

   removing all locking and checking makes a marginal difference:

   4:   0.000017 seconds (201 allocations: 5.859 KiB)
   8:   0.000016 seconds (385 allocations: 11.188 KiB)
   16:   0.000054 seconds (810 allocations: 23.781 KiB)
   32:   0.000091 seconds (1.72 k allocations: 50.953 KiB)
   64:   0.000173 seconds (3.54 k allocations: 105.047 KiB)
   128:   0.000350 seconds (7.17 k allocations: 213.578 KiB)
   256:   0.000709 seconds (14.43 k allocations: 429.953 KiB)
   512:   0.002295 seconds (35.85 k allocations: 986.891 KiB)
   1024:   0.007289 seconds (93.53 k allocations: 2.309 MiB)
   2048:   0.017355 seconds (208.90 k allocations: 5.000 MiB)
   4096:   0.057255 seconds (441.65 k allocations: 10.515 MiB, 20.47% gc time, 13.93% compilation time)
   8192:   0.076030 seconds (901.08 k allocations: 21.144 MiB)
   16384:   1.064113 seconds (11.70 M allocations: 193.909 MiB, 3.54% gc time)
   32768:   4.895505 seconds (53.15 M allocations: 843.174 MiB, 2.19% gc time)

   Curiourly, transients are faster for vectors of depth < 2 and use quite a bit
   less memory until 8k 2^13 nodes, but after that it reverses violently. Something
   strange is happening as the tree grows. I'm computing the count recursively
   instead of storing it. I'm an idiot.

   After storing the count in a field as per the persistent version:

   2:   0.000038 seconds (110 allocations: 3.188 KiB)
   4:   0.000014 seconds (203 allocations: 5.906 KiB)
   8:   0.000020 seconds (387 allocations: 11.234 KiB)
   16:   0.000069 seconds (780 allocations: 22.828 KiB)
   32:   0.000104 seconds (1.57 k allocations: 46.125 KiB)
   64:   0.000197 seconds (3.13 k allocations: 92.469 KiB)
   128:   0.000396 seconds (6.27 k allocations: 185.500 KiB)
   256:   0.000781 seconds (12.54 k allocations: 370.891 KiB)
   512:   0.001643 seconds (27.16 k allocations: 774.953 KiB)
   1024:   0.003485 seconds (58.45 k allocations: 1.577 MiB)
   2048:   0.007230 seconds (121.01 k allocations: 3.218 MiB)
   4096:   0.014666 seconds (246.15 k allocations: 6.500 MiB)
   8192:   0.043788 seconds (496.42 k allocations: 13.064 MiB, 30.56% gc time)
   16384:   0.061483 seconds (1.09 M allocations: 27.645 MiB)
   32768:   0.147103 seconds (2.48 M allocations: 59.745 MiB, 9.11% gc time)
   65536:   0.304465 seconds (5.24 M allocations: 123.945 MiB, 9.01% gc time)
   131072:   0.624119 seconds (10.78 M allocations: 252.346 MiB, 9.83% gc time)
   262144:   1.273259 seconds (21.85 M allocations: 509.146 MiB, 10.89% gc time)
   524288:   2.741977 seconds (47.05 M allocations: 1.044 GiB, 12.38% gc time)
   1048576:   5.857330 seconds (103.72 M allocations: 2.232 GiB, 12.94% gc time)

   We're catching up on time, and we're using *less* ram than Base.Vector. Though
   the difference is negligeble.

   This won't work well for multithreading though. I'll have to compare
   multithreaded persistent vectors with whever I can accomplish here.


   At this point, let's go back and check the comparison:


   #+BEGIN_SRC julia
     for i in 1:k
       print(string(2^i)*": ")
       @time rgb::Vector{UInt8} = reduce(vcat,
                                         map(p -> [
                                           reinterpret(UInt8, p.b), reinterpret(UInt8, p.g), reinterpret(UInt8, p.r),
                                           0xff
                                         ],
                                             image[1:2^i]
                                             )
                                         )
     end
   #+END_SRC

   and we get:

   2:   0.000011 seconds (8 allocations: 448 bytes)
   4:   0.000002 seconds (10 allocations: 624 bytes)
   8:   0.000002 seconds (14 allocations: 976 bytes)
   16:   0.000002 seconds (22 allocations: 1.625 KiB)
   32:   0.000002 seconds (38 allocations: 3.000 KiB)
   64:   0.000004 seconds (70 allocations: 5.734 KiB)
   128:   0.000004 seconds (134 allocations: 11.172 KiB)
   256:   0.000012 seconds (262 allocations: 22.156 KiB)
   512:   0.000015 seconds (518 allocations: 44.078 KiB)
   1024:   0.000028 seconds (1.03 k allocations: 87.609 KiB)
   2048:   0.000054 seconds (2.05 k allocations: 174.609 KiB)
   4096:   0.000102 seconds (4.11 k allocations: 348.375 KiB)
   8192:   0.000203 seconds (8.20 k allocations: 696.297 KiB)
   16384:   0.000520 seconds (16.39 k allocations: 1.360 MiB)
   32768:   0.000909 seconds (32.78 k allocations: 2.719 MiB)
   65536:   0.001815 seconds (65.55 k allocations: 5.438 MiB)
   131072:   0.003632 seconds (131.08 k allocations: 10.875 MiB)
   262144:   0.024924 seconds (262.15 k allocations: 21.750 MiB, 57.98% gc time)
   524288:   0.015855 seconds (524.30 k allocations: 43.500 MiB)
   1048576:   0.066121 seconds (1.05 M allocations: 87.000 MiB, 33.43% gc time)

   I'm so far off it's embarrassing. I must have included compile time without
   checking it before.

   So I'm still 2 orders of magnitude off on time and one on space...

   Special methods for creating small vectors make a noticable improvement:

   2:   0.000019 seconds (29 allocations: 944 bytes)
   4:   0.000005 seconds (40 allocations: 1.359 KiB)
   8:   0.000006 seconds (60 allocations: 2.125 KiB)
   16:   0.000034 seconds (125 allocations: 4.594 KiB)
   32:   0.000043 seconds (256 allocations: 9.641 KiB)
   64:   0.000078 seconds (512 allocations: 19.484 KiB)
   128:   0.000168 seconds (1.02 k allocations: 39.516 KiB)
   256:   0.000831 seconds (2.05 k allocations: 78.906 KiB)
   512:   0.000700 seconds (6.17 k allocations: 190.969 KiB)
   1024:   0.001800 seconds (16.46 k allocations: 447.203 KiB)
   2048:   0.003910 seconds (37.05 k allocations: 959.453 KiB)
   4096:   0.008681 seconds (78.22 k allocations: 1.938 MiB)
   8192:   0.015827 seconds (160.55 k allocations: 3.939 MiB)
   16384:   0.036072 seconds (420.48 k allocations: 9.395 MiB)
   32768:   0.075162 seconds (1.13 M allocations: 23.245 MiB)
   65536:   0.157719 seconds (2.56 M allocations: 50.945 MiB)
   131072:   0.339623 seconds (5.41 M allocations: 106.346 MiB)
   262144:   0.701219 seconds (11.11 M allocations: 217.146 MiB, 8.55% gc time)
   524288:   1.494626 seconds (25.55 M allocations: 485.249 MiB, 7.36% gc time)
   1048576:   3.373203 seconds (60.73 M allocations: 1.091 GiB, 10.46% gc time)

   Halving the ram usage. Not bad.

** [2023-10-01 Sun] More focused metrics

   These notes are a mess. But then so are my thoughts at the moment.

   Let's try another tack.

   Naive =into=
   #+BEGIN_SRC julia
     @time into(emptyvector, 1:2^20)
     1.649648 seconds (30.95 M allocations: 3.548 GiB, 26.17% gc time)
   #+END_SRC

   Transient =into=
   #+BEGIN_SRC julia
     @time into(emptyvector, 1:2^20)
     0.715259 seconds (17.95 M allocations: 318.826 MiB, 15.18% gc time)
   #+END_SRC

   alloc once recursive partitioning (current =vec= implementation).
   #+BEGIN_SRC julia
     @time vec(1:2^20)
     0.148071 seconds (3.40 M allocations: 98.187 MiB, 32.71% gc time)
   #+END_SRC

   The moral being that while transients help — a lot! — being clever helps even
   more.

   Of course, compared to native methods we're still out in the woods:

   #+BEGIN_SRC julia
     @time [i for i in 1:2^20]; nothing
     0.001074 seconds (2 allocations: 8.000 MiB)
   #+END_SRC

   So we're 100x slower and 10x heavier on ram than julia's datastructures. I'm
   pretty sure further optimisation of the transients is beyond me without moving
   to a lower level.

   Curiously, preallocating the accumulator in =leafpartition= and using an index
   (ring buffer) uses *more* ram (1.5x) but reduces runtime by a third.

   #+BEGIN_SRC julia
     function leafpartition(T)
       acc = Base.Vector{T}(undef, nodelength)
       i = 0
       function (emit)
         function inner()
           emit()
         end
         function inner(result)
           if i > 0
             emit(emit(result, [acc[j] for j in 1:i]))
           else
             emit(result)
           end
         end
         function inner(result, next)
           i+= 1
           acc[i] = next
           if i == nodelength
             t = copy(acc)
             i = 0
             emit(result, t)
           else
             result
           end
         end
         return inner
       end
     end

     @time  vec(1:2^20); nothing
     0.093256 seconds (3.37 M allocations: 144.004 MiB, 35.41% gc time)
   #+END_SRC

   Given the way vectors normally auto resize 8->40->..., I would have thought
   that we'd be saving ram this way. Odd. But the speedup is probably because
   there are more known types.

   Nope. Removing the preallocated buffer, but keeping the type argument makes
   everything worse...

   I'm running the measures again and it looks like the ring buffer method uses
   <10% more ram and runs 4x faster.

   I need a better test setup than @time and @profile...
** [2023-10-02 Mon] Times to beat

  #+BEGIN_SRC julia
    @time vec(1:2^20); nothing
    0.073854 seconds (3.37 M allocations: 144.005 MiB, 40.62% gc time)

    @time into(emptyvector, 1:2^20); nothing
    0.359100 seconds (11.55 M allocations: 207.188 MiB, 11.02% gc time)
  #+END_SRC
** Parallel transduction
   Stateless transducers are trivially parallel. Many stateful transducers can
   also be made parallel, but this is trickier. For now we can just rely on
   programmer annotations to tell us when a transduction is associative.

   A pipeline is associative if all components are.

   Reduction is harder because concat is a performance killer in the current
   persistent vector implementation.

   RRB Tries could solve this problem. I haven't read that paper in a long time
   though.

   Since merge on maps is associative and commutative, we ought to be able to
   parallelise mapish operations without too much ado.

   Now do I really want to try and implement something along the lines of Cilk?
** history of a waste of time
   I just removed the depth parameter from VectorNodes to see if the space
   savings (N/31 bytes for N elements in large vectors) was worth it. Why?
   Because I got carried away. Turns out I carried the depth around because I
   needed it to avoid doing a bunch of logarithms in nth and conj.

   Times for creation/nth/conj before "improvement"

   0.178549 seconds (8.79 M allocations: 503.427 MiB, 12.87% gc time)
   0.000755 seconds (22.00 k allocations: 453.109 KiB)
   0.000498 seconds (16.49 k allocations: 867.016 KiB)

   And times after:

   0.195730 seconds (8.79 M allocations: 503.385 MiB, 18.36% gc time)
   0.001861 seconds (32.44 k allocations: 930.531 KiB)
   0.000568 seconds (21.49 k allocations: 851.391 KiB)

   It actually needs more allocations to save a marginal amount of space.

   I just reverted all changes locally. I should have kept them in git for my own
   future reference. Should I have?
** Overriding =into= for =EmptyVector=
   I feel like an idiot for not thinking of this before. (into [] xs) is the same
   as vec(xs), so why not dispatch that way?

   standard benchmark:
   2:   0.000073 seconds (80 allocations: 2.625 KiB)
   4:   0.000020 seconds (126 allocations: 4.062 KiB)
   8:   0.000017 seconds (216 allocations: 6.875 KiB)
   16:   0.000031 seconds (570 allocations: 15.250 KiB)
   32:   0.000038 seconds (1.28 k allocations: 32.016 KiB)
   64:   0.000119 seconds (1.70 k allocations: 80.125 KiB)
   128:   0.000135 seconds (3.34 k allocations: 156.766 KiB)
   256:   0.000242 seconds (6.59 k allocations: 311.406 KiB)
   512:   0.000492 seconds (16.99 k allocations: 718.578 KiB)
   1024:   0.000990 seconds (37.79 k allocations: 1.497 MiB)
   2048:   0.002113 seconds (52.35 k allocations: 2.670 MiB)
   4096:   0.004327 seconds (104.62 k allocations: 5.345 MiB)
   8192:   0.008454 seconds (209.11 k allocations: 10.704 MiB)
   16384:   0.022399 seconds (542.04 k allocations: 25.210 MiB, 21.70% gc time)
   32768:   0.037470 seconds (1.21 M allocations: 54.223 MiB, 11.95% gc time)
   65536:   0.085009 seconds (1.75 M allocations: 121.065 MiB, 14.83% gc time)
   131072:   0.169703 seconds (3.49 M allocations: 242.178 MiB, 12.29% gc time)
   262144:   0.348428 seconds (6.98 M allocations: 484.417 MiB, 13.00% gc time)
   524288:   0.690695 seconds (18.75 M allocations: 1.087 GiB, 13.53% gc time)
   1048576:   1.360049 seconds (42.30 M allocations: 2.314 GiB, 13.18% gc time)

   vs transients:

   2:   0.000038 seconds (81 allocations: 2.375 KiB)
   4:   0.000015 seconds (142 allocations: 4.141 KiB)
   8:   0.000011 seconds (262 allocations: 7.547 KiB)
   16:   0.000055 seconds (525 allocations: 15.328 KiB)
   32:   0.000056 seconds (1.02 k allocations: 29.344 KiB)
   64:   0.000085 seconds (2.00 k allocations: 57.938 KiB)
   128:   0.000158 seconds (3.96 k allocations: 114.047 KiB)
   256:   0.000315 seconds (8.39 k allocations: 235.531 KiB)
   512:   0.000735 seconds (19.34 k allocations: 512.375 KiB)
   1024:   0.001510 seconds (41.21 k allocations: 1.040 MiB)
   2048:   0.003107 seconds (84.94 k allocations: 2.103 MiB)
   4096:   0.006271 seconds (172.41 k allocations: 4.255 MiB)
   8192:   0.012618 seconds (347.33 k allocations: 8.566 MiB)
   16384:   0.036978 seconds (794.50 k allocations: 18.686 MiB, 22.71% gc time)
   32768:   0.066359 seconds (1.69 M allocations: 38.665 MiB, 11.21% gc time)
   65536:   0.126810 seconds (3.48 M allocations: 78.894 MiB, 5.65% gc time)
   131072:   0.273211 seconds (7.05 M allocations: 159.365 MiB, 10.78% gc time)
   262144:   0.545332 seconds (14.21 M allocations: 320.323 MiB, 10.57% gc time)
   524288:   1.211752 seconds (31.66 M allocations: 689.117 MiB, 11.48% gc time)
   1048576:   2.509304 seconds (66.57 M allocations: 1.393 GiB, 12.58% gc time)

   These are both so much beter than the naive persistent reduction I won't even
   add it in.

   So the tranduction builder is about twice as fast and makes fewer allocations,
   but uses more scratch space. But the speedup is after the additional GC
   overhead.

   Ahh, there's an insidious bug in the above: the transform pipeline that builds
   the vector is based on the length of the input, but the prior pipeline is 4:1,
   thus the builder pipeline can only accomodate 1/4 of the data.

   Fixing this by hardcoding the correct number of layers gives us:

   2:   0.000152 seconds (177 allocations: 11.484 KiB)
   4:   0.000067 seconds (221 allocations: 15.281 KiB)
   8:   0.000065 seconds (304 allocations: 22.766 KiB)
   16:   0.000084 seconds (498 allocations: 39.703 KiB)
   32:   0.000097 seconds (860 allocations: 70.516 KiB)
   64:   0.000119 seconds (1.58 k allocations: 132.859 KiB)
   128:   0.000176 seconds (3.03 k allocations: 256.125 KiB)
   256:   0.000284 seconds (5.89 k allocations: 503.984 KiB)
   512:   0.000522 seconds (11.67 k allocations: 1002.172 KiB)
   1024:   0.000964 seconds (23.20 k allocations: 1.950 MiB)
   2048:   0.001869 seconds (46.26 k allocations: 3.874 MiB)
   4096:   0.007182 seconds (92.38 k allocations: 7.749 MiB, 48.77% gc time)
   8192:   0.007224 seconds (184.59 k allocations: 15.507 MiB)
   16384:   0.018153 seconds (369.05 k allocations: 31.036 MiB, 20.55% gc time)
   32768:   0.032288 seconds (737.96 k allocations: 61.833 MiB, 11.53% gc time)
   65536:   0.067171 seconds (1.48 M allocations: 123.697 MiB, 15.47% gc time)
   131072:   0.133401 seconds (2.95 M allocations: 247.439 MiB, 14.19% gc time)
   262144:   0.262998 seconds (5.90 M allocations: 494.937 MiB, 13.02% gc time)
   524288:   0.525131 seconds (11.81 M allocations: 988.827 M
   1048576:   1.052119 seconds (23.61 M allocations: 1.930 GiB, 13.67% gc time)

   which is a considerable improvement.

   Unfortunately, I'm at something of a loss as to how to dynamically create the
   correct pipeline. The problem with transducers is that once they start,
   they're locked because the reducer is passed in and so you can't add more
   steps at the end without unapplying that reducing function.

   Maybe I can create a clever reducing function...

   ...And a clever reducing function is the secret. *But* you have to make sure
   all state in stateful transducers is stored in the first inner closure, that
   way you can repeatedly apply the reducer without clearing the state.

   New times:

   2:   0.000065 seconds (100 allocations: 3.656 KiB)
   4:   0.000015 seconds (150 allocations: 5.453 KiB)
   8:   0.000018 seconds (246 allocations: 8.969 KiB)
   16:   0.000021 seconds (453 allocations: 16.469 KiB)
   32:   0.000036 seconds (867 allocations: 31.188 KiB)
   64:   0.000062 seconds (1.70 k allocations: 61.344 KiB)
   128:   0.000120 seconds (3.35 k allocations: 120.250 KiB)
   256:   0.000244 seconds (6.65 k allocations: 240.906 KiB)
   512:   0.000463 seconds (13.31 k allocations: 483.188 KiB)
   1024:   0.000912 seconds (26.63 k allocations: 968.234 KiB)
   2048:   0.001819 seconds (53.28 k allocations: 1.876 MiB)
   4096:   0.003708 seconds (106.57 k allocations: 3.762 MiB)
   8192:   0.007357 seconds (213.13 k allocations: 7.544 MiB)
   16384:   0.018811 seconds (428.30 k allocations: 15.259 MiB, 21.92% gc time)
   32768:   0.029789 seconds (858.64 k allocations: 30.430 MiB)
   65536:   0.067130 seconds (1.72 M allocations: 61.042 MiB, 11.79% gc time)
   131072:   0.129509 seconds (3.44 M allocations: 122.280 MiB, 8.24% gc time)
   262144:   0.256636 seconds (6.88 M allocations: 244.771 MiB, 7.31% gc time)
   524288:   0.527511 seconds (13.83 M allocations: 494.662 MiB, 8.60% gc time)
   1048576:   1.049643 seconds (27.74 M allocations: 994.445 MiB, 8.07% gc time)

   which is now faster than transients and use less ram. Given there's also a bug
   in transients that leads to a use after free (presumably something isn't being
   copied properly), I'm tempted to just cut them out entirely. They have some
   uses though.

   The times above involve calling transduce with a fixed xform and a funky
   reducer, but why not just call reduce? Effect:

   2:   0.000062 seconds (87 allocations: 2.766 KiB)
   4:   0.000015 seconds (145 allocations: 4.547 KiB)
   8:   0.000036 seconds (275 allocations: 8.938 KiB)
   16:   0.000046 seconds (530 allocations: 20.469 KiB)
   32:   0.000067 seconds (1.04 k allocations: 43.125 KiB)
   64:   0.000126 seconds (2.05 k allocations: 89.156 KiB)
   128:   0.000243 seconds (4.07 k allocations: 179.797 KiB)
   256:   0.000497 seconds (8.10 k allocations: 364.062 KiB)
   512:   0.001089 seconds (16.69 k allocations: 839.438 KiB)
   1024:   0.002231 seconds (33.85 k allocations: 1.749 MiB)
   2048:   0.004557 seconds (68.18 k allocations: 3.589 MiB)
   4096:   0.009160 seconds (136.84 k allocations: 7.296 MiB)
   8192:   0.019003 seconds (274.13 k allocations: 14.720 MiB)
   16384:   0.042296 seconds (581.53 k allocations: 36.219 MiB, 11.67% gc time)
   32768:   0.086952 seconds (1.20 M allocations: 78.958 MiB, 10.31% gc time)
   65536:   0.174166 seconds (2.43 M allocations: 164.707 MiB, 8.94% gc time)
   131072:   0.343993 seconds (4.89 M allocations: 336.218 MiB, 7.48% gc time)
   262144:   0.695022 seconds (9.80 M allocations: 679.257 MiB, 8.00% gc time)
   524288:   1.589509 seconds (20.16 M allocations: 1.470 GiB, 9.17% gc time)
   1048576:   3.353290 seconds (40.89 M allocations: 3.083 GiB, 10.07% gc time)

   What the hell?

   Hypothesis: since each step gets called 32x less than the step before it, the
   gains of caching xf(r) are tiny at the end, but huge in the beginning. So use
   a fixed pipeline as much as possible and go dynamic only at need.

   Just caching =tailxform(lastarg)= instead of apply it at every step gets us:

   2:   0.000077 seconds (98 allocations: 3.781 KiB)
   4:   0.000015 seconds (144 allocations: 5.578 KiB)
   8:   0.000017 seconds (230 allocations: 9.031 KiB)
   16:   0.000022 seconds (422 allocations: 16.578 KiB)
   32:   0.000036 seconds (804 allocations: 31.328 KiB)
   64:   0.000066 seconds (1.57 k allocations: 61.547 KiB)
   128:   0.000127 seconds (3.10 k allocations: 120.562 KiB)
   256:   0.000265 seconds (6.14 k allocations: 241.531 KiB)
   512:   0.000488 seconds (12.23 k allocations: 481.406 KiB)
   1024:   0.000967 seconds (24.40 k allocations: 961.516 KiB)
   2048:   0.001890 seconds (48.74 k allocations: 1.860 MiB)
   4096:   0.003788 seconds (97.43 k allocations: 3.727 MiB)
   8192:   0.007581 seconds (194.80 k allocations: 7.471 MiB)
   16384:   0.019740 seconds (389.52 k allocations: 14.968 MiB, 23.22% gc time)
   32768:   0.030088 seconds (778.97 k allocations: 29.703 MiB)
   65536:   0.070219 seconds (1.56 M allocations: 59.444 MiB, 13.97% gc time)
   131072:   0.129730 seconds (3.12 M allocations: 118.940 MiB, 7.22% gc time)
   262144:   0.269297 seconds (6.23 M allocations: 237.948 MiB, 10.41% gc time)
   524288:   0.531737 seconds (12.46 M allocations: 474.870 MiB, 9.54% gc time)
   1048576:   1.058747 seconds (24.92 M allocations: 948.718 MiB, 9.15% gc time)

   5% memory savings, essentially the same times. I need a more deterministic
   harness.

   After cleaning up some corner case bugs around aborted transduction, the
   memory requirements are back where they were, but it's faster. Go figure:

   2:   0.000076 seconds (105 allocations: 3.922 KiB)
   4:   0.000018 seconds (159 allocations: 5.859 KiB)
   8:   0.000016 seconds (262 allocations: 9.594 KiB)
   16:   0.000025 seconds (486 allocations: 17.641 KiB)
   32:   0.000036 seconds (930 allocations: 33.328 KiB)
   64:   0.000074 seconds (1.82 k allocations: 65.422 KiB)
   128:   0.000122 seconds (3.60 k allocations: 128.188 KiB)
   256:   0.000263 seconds (7.13 k allocations: 256.562 KiB)
   512:   0.000514 seconds (14.21 k allocations: 511.406 KiB)
   1024:   0.000936 seconds (28.36 k allocations: 1021.453 KiB)
   2048:   0.001871 seconds (56.67 k allocations: 1.977 MiB)
   4096:   0.003713 seconds (113.29 k allocations: 3.961 MiB)
   8192:   0.011464 seconds (226.51 k allocations: 7.938 MiB, 35.49% gc time)
   16384:   0.014529 seconds (452.94 k allocations: 15.903 MiB)
   32768:   0.033638 seconds (905.82 k allocations: 31.574 MiB, 12.76% gc time)
   65536:   0.062521 seconds (1.81 M allocations: 63.186 MiB, 6.75% gc time)
   131072:   0.128986 seconds (3.62 M allocations: 126.424 MiB, 9.16% gc time)
   262144:   0.260248 seconds (7.25 M allocations: 252.915 MiB, 9.14% gc time)
   524288:   0.516125 seconds (14.49 M allocations: 504.806 MiB, 8.50% gc time)
   1048576:   1.030130 seconds (28.98 M allocations: 1008.589 MiB, 8.89% gc time)
** Fast Maps
   Maps have until recently been "good enough". I'm starting to get some issues
   with looking up elements in maps in a render loop being too much overhead. I
   will probably switch those hotspots to structs eventually, but I still need
   to fix this.

   First of all, there should be no allocation when looking up a key in a map. I
   should also start using typed map entries so that values can be inlined as
   much as possible. Keys are overwhelmingly often of a predictable, homogeneous
   type, so let's use that.
*** Baseline
**** Array maps
***** construction
      julia> @benchmark reduce(conj, emptymap, [[1,2], [3,4], [5,6], [7,8]])
      BenchmarkTools.Trial: 10000 samples with 192 evaluations.
      Range (min … max):  515.318 ns …   8.651 μs  ┊ GC (min … max): 0.00% … 87.83%
      Time  (median):     536.359 ns               ┊ GC (median):    0.00%
      Time  (mean ± σ):   568.772 ns ± 426.706 ns  ┊ GC (mean ± σ):  5.00% ±  6.13%

      ▁▂▂▄▇█▆
      ▂▃▄▅▇████████▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂ ▃
      515 ns           Histogram: frequency by time          648 ns <

      Memory estimate: 1.11 KiB, allocs estimate: 20.

      julia> @benchmark into(emptymap, partition(2), 1:8)
      BenchmarkTools.Trial: 10000 samples with 10 evaluations.
      Range (min … max):  1.608 μs … 220.839 μs  ┊ GC (min … max): 0.00% … 98.05%
      Time  (median):     1.675 μs               ┊ GC (median):    0.00%
      Time  (mean ± σ):   1.820 μs ±   4.113 μs  ┊ GC (mean ± σ):  4.46% ±  1.96%

      ▂▆██▇▆▅▅▄▄▃▂▁                                ▁▁             ▂
      ██████████████▆▇▆▆▅▆▅▅▅▆▆▅▇▆▅▅▄▄▄▅▇██▇▇▆▄▅▄▇█████▆▅▅▆▇▇█▇▆▅ █
      1.61 μs      Histogram: log(frequency) by time       2.6 μs <

      Memory estimate: 1.86 KiB, allocs estimate: 51.

      julia> @benchmark into(emptymap, partition(2), 1:32)
      BenchmarkTools.Trial: 10000 samples with 3 evaluations.
      Range (min … max):  7.993 μs … 485.922 μs  ┊ GC (min … max): 0.00% … 96.16%
      Time  (median):     8.483 μs               ┊ GC (median):    0.00%
      Time  (mean ± σ):   9.253 μs ±  12.640 μs  ┊ GC (mean ± σ):  4.21% ±  3.04%

      ▄▆███▇▆▄▃                             ▁                    ▂
      ███████████▇▆▅▆▆▆▆▅▄▅▃▅▅▅▅▄▆▅▅▅▇▇▆███████████▇██▇█▇▇▇▇▇▇▆▇▆ █
      7.99 μs      Histogram: log(frequency) by time      13.7 μs <

      Memory estimate: 14.94 KiB, allocs estimate: 195.

      clearly need to override `into` for empty maps.
***** get
      julia> m = into(emptymap, partition(2), 1:32)

      julia> @benchmark get(m, 1)
      BenchmarkTools.Trial: 10000 samples with 995 evaluations.
      Range (min … max):  28.020 ns … 55.358 ns  ┊ GC (min … max): 0.00% … 0.00%
      Time  (median):     28.775 ns              ┊ GC (median):    0.00%
      Time  (mean ± σ):   28.933 ns ±  1.543 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%

      ▄    █                                                      ▁
      █▁▁▁▁█▃▁▁▁▁▁▃▁▃▄▄▄▄▇▆▆▅▅▆▆▄▄▃▃▃▁▁▁▁▃▁▁▁▁▁▃▃▁▁▁▃▁▁▄▁▄▁▆█▅▅▆▆ █
      28 ns        Histogram: log(frequency) by time      36.1 ns <

      Memory estimate: 0 bytes, allocs estimate: 0.

      julia> @benchmark get(m, 15)
      BenchmarkTools.Trial: 10000 samples with 923 evaluations.
      Range (min … max):  112.775 ns … 397.827 ns  ┊ GC (min … max): 0.00% … 0.00%
      Time  (median):     115.277 ns               ┊ GC (median):    0.00%
      Time  (mean ± σ):   115.034 ns ±   4.079 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%

      ▅   █        █                            ▁         ▁         ▂
      █▁▁▃█▁▃▁▄▅▄▅▅███▇█▆▆▇▆▅▆███▇▆▇▇▆▅▄▄▄▁▃▅▅▄▄█████▇▆▇▇█████▇▇▇█▇ █
      113 ns        Histogram: log(frequency) by time        124 ns <

      Memory estimate: 0 bytes, allocs estimate: 0.

      julia> @benchmark get(m, 31)
      BenchmarkTools.Trial: 10000 samples with 516 evaluations.
      Range (min … max):  216.359 ns … 342.446 ns  ┊ GC (min … max): 0.00% … 0.00%
      Time  (median):     222.696 ns               ┊ GC (median):    0.00%
      Time  (mean ± σ):   223.888 ns ±   4.282 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%

      ▁▁      ▁▃▅▆██▅▅▃▁    ▁▁▁ ▁                   ▁▁▂▂▁▁▁▁    ▂
      ▄▄▅▇██▇▇▄▄▇▆██████████▅▇▇███████▆▆▅▄▄▄▄▅▄▄▃▁▄▁▁▅▇▇█████████▇▇ █
      216 ns        Histogram: log(frequency) by time        239 ns <

      Memory estimate: 0 bytes, allocs estimate: 0.

      Not bad, but I'll reckon we can do better.
***** assoc
      m = hashmap(1,1)

      julia> @benchmark assoc(m, 2, 2)
      BenchmarkTools.Trial: 10000 samples with 957 evaluations.
      Range (min … max):   89.771 ns …  2.026 μs  ┊ GC (min … max): 0.00% … 91.89%
      Time  (median):      93.470 ns              ┊ GC (median):    0.00%
      Time  (mean ± σ):   100.064 ns ± 91.065 ns  ┊ GC (mean ± σ):  5.37% ±  5.56%

      ▅▆▇█▇▄▃▂▁  ▁▁▂▂▂▁                                           ▂
      ███████████████████▇▇▆▆▄▄▃▁▃▄▁▁▁▄▁▁▁▃▃▁▁▁▁▁▃▁▁▃▁▁▃▁▁▁▄▅▆▇▆▇▆ █
      89.8 ns       Histogram: log(frequency) by time       140 ns <

      Memory estimate: 176 bytes, allocs estimate: 4.

      m = into(emptymap, partition(2), 1:16)

      julia> @benchmark assoc(m, 17, 18)
      BenchmarkTools.Trial: 10000 samples with 651 evaluations.
      Range (min … max):  185.562 ns …  1.175 μs  ┊ GC (min … max): 0.00% … 76.18%
      Time  (median):     196.069 ns              ┊ GC (median):    0.00%
      Time  (mean ± σ):   207.018 ns ± 64.560 ns  ┊ GC (mean ± σ):  3.14% ±  7.77%

      ▇█
      ██▆▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂ ▂
      186 ns          Histogram: frequency by time          713 ns <

      Memory estimate: 880 bytes, allocs estimate: 4.

      julia> @benchmark assoc(m, 3, 5)
      BenchmarkTools.Trial: 10000 samples with 383 evaluations.
      Range (min … max):  249.454 ns …   3.443 μs  ┊ GC (min … max): 0.00% … 90.92%
      Time  (median):     257.235 ns               ┊ GC (median):    0.00%
      Time  (mean ± σ):   276.523 ns ± 218.079 ns  ┊ GC (mean ± σ):  6.06% ±  7.01%

      ▂▄▆▇████▇▇▆▅▅▄▄▃▃▂▂▂▁▁ ▁      ▁▁▂▃▂▁▁▁▁  ▁                  ▃
      ▅▇██████████████████████████▇█▇█████████████▇▇▇▇▇▇▆▆▇▆▆▆▄▅▃▁▅ █
      249 ns        Histogram: log(frequency) by time        301 ns <

      Memory estimate: 736 bytes, allocs estimate: 8.

      So appending at the end is much faster than changing the middle. That
      shouldn't be such a difference.

      m = into(emptymap, partition(2), 1:32)

      julia> @benchmark assoc(m, 15, 11)
      BenchmarkTools.Trial: 10000 samples with 225 evaluations.
      Range (min … max):  334.227 ns …   5.012 μs  ┊ GC (min … max): 0.00% … 87.13%
      Time  (median):     343.733 ns               ┊ GC (median):    0.00%
      Time  (mean ± σ):   364.682 ns ± 221.426 ns  ┊ GC (mean ± σ):  4.00% ±  6.02%

      ▅█▆▃▂  ▁▂▂                                                    ▁
      ██████▆████▇▇▅▅▄▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▃▅▆▆▆▅▄▃▃▃▁▄ █
      334 ns        Histogram: log(frequency) by time        680 ns <

      Memory estimate: 976 bytes, allocs estimate: 8.

      julia> @benchmark assoc(m, :a, 0)
      BenchmarkTools.Trial: 10000 samples with 4 evaluations.
      Range (min … max):  7.058 μs … 228.857 μs  ┊ GC (min … max): 0.00% … 93.52%
      Time  (median):     7.353 μs               ┊ GC (median):    0.00%
      Time  (mean ± σ):   7.681 μs ±   4.941 μs  ┊ GC (mean ± σ):  1.60% ±  2.47%

      ▃▆██▇▅▂                                                     ▂
      ███████▇▆▇▇▆▇▆▅▅▅▄▅▃▂▄▄▃▄▄▆▇▇█▅▆▆▄▅▅▅▅▆▄▅▅▆▆▅▄▆▆▅▅▆▇▆▄▄▅▄▄▅ █
      7.06 μs      Histogram: log(frequency) by time      12.3 μs <

      Memory estimate: 9.52 KiB, allocs estimate: 177.

      Upgrading to hashmaps is a disaster. There's also too much allocation
      happening at every stage here.
***** merge
      m1 = into(emptymap, partition(2), 1:16)
      m2 = into(emptymap, partition(2), 17:32)
      m3 = into(emptymap, partition(2), 9:22)

      julia> @benchmark merge(m1, m2)
      BenchmarkTools.Trial: 10000 samples with 9 evaluations.
      Range (min … max):  2.636 μs … 68.770 μs  ┊ GC (min … max): 0.00% … 92.24%
      Time  (median):     3.000 μs              ┊ GC (median):    0.00%
      Time  (mean ± σ):   3.211 μs ±  2.266 μs  ┊ GC (mean ± σ):  2.58% ±  3.65%

      ▃▅▅▇██▇▄▁                                                 ▂
      ██████████▇▇▇▆▆▃▃▅▅▆▇█▇▇▇▇▆▅▄▄▄▄▃▄▄▃▃▁▃▄▁▄▅▄▄▆▇▇█▇█████▇█▇ █
      2.64 μs      Histogram: log(frequency) by time     6.24 μs <

      Memory estimate: 8.75 KiB, allocs estimate: 54.

      julia> @benchmark merge(m1, m3)
      BenchmarkTools.Trial: 10000 samples with 9 evaluations.
      Range (min … max):  2.541 μs … 140.629 μs  ┊ GC (min … max): 0.00% … 95.10%
      Time  (median):     2.739 μs               ┊ GC (median):    0.00%
      Time  (mean ± σ):   2.976 μs ±   3.853 μs  ┊ GC (mean ± σ):  4.32% ±  3.30%

      ▃▆▇█▇▆▅▃▂▁                                                 ▂
      ▇███████████▇▇▇▆▆▄▅▅▃▁▃▃▁▁▅▇▇█▇▆▆▆▆▅▄▄▅▄▄▅▆▅▄▇▇▇█▇▇███▇▇▇▆▇ █
      2.54 μs      Histogram: log(frequency) by time      5.02 μs <

      Memory estimate: 6.31 KiB, allocs estimate: 64.

      So merge performance is abominable, but seems to be independent of
      intersection testing.
**** Hash maps
***** construction
      julia> @benchmark into(emptyhashmap, partition(2), 1:16)
      BenchmarkTools.Trial: 10000 samples with 5 evaluations.
      Range (min … max):  6.046 μs … 255.145 μs  ┊ GC (min … max): 0.00% … 95.89%
      Time  (median):     6.360 μs               ┊ GC (median):    0.00%
      Time  (mean ± σ):   6.755 μs ±   6.458 μs  ┊ GC (mean ± σ):  2.63% ±  2.70%

      ▄▇▇██▇▆▅▃▂▁                                      ▁▁▂ ▁▁▂▁▁ ▂
      ████████████▇█▇█▇█▇▆▆▆▅▆▅▅▅▅▁▄▁▆▆▆▇██▇█▇▅▆▆▄▇█▇▇▇▇█████████ █
      6.05 μs      Histogram: log(frequency) by time      9.54 μs <

      Memory estimate: 7.31 KiB, allocs estimate: 170.

      julia> @benchmark into(emptyhashmap, partition(2), 1:32)
      BenchmarkTools.Trial: 10000 samples with 1 evaluation.
      Range (min … max):  11.890 μs …  1.276 ms  ┊ GC (min … max): 0.00% … 97.44%
      Time  (median):     12.630 μs              ┊ GC (median):    0.00%
      Time  (mean ± σ):   13.206 μs ± 20.433 μs  ┊ GC (mean ± σ):  2.62% ±  1.68%

      ▃▇█▅▁
      ▂▄█████▆▄▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂ ▃
      11.9 μs         Histogram: frequency by time        21.2 μs <

      Memory estimate: 14.31 KiB, allocs estimate: 330.

      Way too much allocation.
***** get
      julia> @benchmark get(m, 1)
      BenchmarkTools.Trial: 10000 samples with 379 evaluations.
      Range (min … max):  248.050 ns …  2.532 μs  ┊ GC (min … max): 0.00% … 87.96%
      Time  (median):     252.456 ns              ┊ GC (median):    0.00%
      Time  (mean ± σ):   256.135 ns ± 55.794 ns  ┊ GC (mean ± σ):  0.59% ±  2.44%

      ▁▂▄▆▇██▇▆▆▆▅▃▁   ▁▁▁▁                     ▁▂▂▁▁▁▁           ▃
      ▆███████████████▇███████▇▇▆▅▆▄▅▅▁▄▅▃▁▅▅▅▆▇▇████████████▇▇▆▆▇ █
      248 ns        Histogram: log(frequency) by time       282 ns <

      Memory estimate: 96 bytes, allocs estimate: 4.

      julia> @benchmark get(m, 31)
      BenchmarkTools.Trial: 10000 samples with 372 evaluations.
      Range (min … max):  249.194 ns …  2.471 μs  ┊ GC (min … max): 0.00% … 87.23%
      Time  (median):     255.110 ns              ┊ GC (median):    0.00%
      Time  (mean ± σ):   257.698 ns ± 52.549 ns  ┊ GC (mean ± σ):  0.52% ±  2.28%

      ▂▃▄▅▆▇▇▇▇███▇▅▄▂▁                              ▁▁▁ ▁        ▃
      ▆███████████████████▇███████▆▇▆▅▃▆▄▄▂▅▃▂▂▅▆▇▇██████████▇▇██▇ █
      249 ns        Histogram: log(frequency) by time       283 ns <

      Memory estimate: 96 bytes, allocs estimate: 4.
***** assoc
      julia> @benchmark assoc(m, 17, 4)
      BenchmarkTools.Trial: 10000 samples with 193 evaluations.
      Range (min … max):  506.016 ns …   5.230 μs  ┊ GC (min … max): 0.00% … 83.37%
      Time  (median):     531.767 ns               ┊ GC (median):    0.00%
      Time  (mean ± σ):   547.224 ns ± 182.230 ns  ┊ GC (mean ± σ):  1.59% ±  4.22%

      ▂▄▆▇██▇▄▃▂   ▂▂▂▃▂▁                                          ▂
      ▇███████████▇████████▇▇▆▅▄▆▆▄▅▅▆▇▅▅▅▃▃▄▃▅▆▇▅▁▅▄▃▄▁▃▁▁▄▃▁▁▅▃▇▇ █
      506 ns        Histogram: log(frequency) by time        769 ns <

      Memory estimate: 592 bytes, allocs estimate: 12.

      julia> @benchmark assoc(m, 170, 4)
      BenchmarkTools.Trial: 10000 samples with 200 evaluations.
      Range (min … max):  406.805 ns …   4.930 μs  ┊ GC (min … max): 0.00% … 84.87%
      Time  (median):     425.055 ns               ┊ GC (median):    0.00%
      Time  (mean ± σ):   435.945 ns ± 170.479 ns  ┊ GC (mean ± σ):  1.85% ±  4.23%

      ▁▂▄▅███▇▄▂
      ▁▁▁▁▂▃▄▅▇██████████▇▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁ ▃
      407 ns           Histogram: frequency by time          483 ns <

      Memory estimate: 560 bytes, allocs estimate: 11.
***** merge
      I don't even want to talk about it
*** Typed Map Entries
    Just make MapEntry, MapEntry{K, V}. That's enough for substantial gain.
**** arraymap get
     BenchmarkTools.Trial: 10000 samples with 998 evaluations.
     Range (min … max):  14.519 ns … 29.069 ns  ┊ GC (min … max): 0.00% … 0.00%
     Time  (median):     14.529 ns              ┊ GC (median):    0.00%
     Time  (mean ± σ):   14.611 ns ±  0.765 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%

     █
     █▂▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂▁▁▁▂▂▂▂▂▂▂ ▂
     14.5 ns         Histogram: frequency by time        17.4 ns <

     Memory estimate: 0 bytes, allocs estimate: 0.

     julia> @benchmark get(m, 15)
     BenchmarkTools.Trial: 10000 samples with 998 evaluations.
     Range (min … max):  17.024 ns … 32.505 ns  ┊ GC (min … max): 0.00% … 0.00%
     Time  (median):     17.285 ns              ┊ GC (median):    0.00%
     Time  (mean ± σ):   17.394 ns ±  0.833 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%

     █ ▄                                                       ▁
     █▁█▁█▃▃▁▁▁▄▃▁▁▄▃▄▅▆▄▅▅▄▃▄▅▄▄▃▃▃▁▁▁▁▃▁▁▁▁▁▁▁▃▁▁▃▁▁▄▁▁▁▃▁▁▁▄▅ █
     17 ns        Histogram: log(frequency) by time      23.9 ns <

     Memory estimate: 0 bytes, allocs estimate: 0.

     julia> @benchmark get(m, 31)
     BenchmarkTools.Trial: 10000 samples with 997 evaluations.
     Range (min … max):  18.766 ns … 108.276 ns  ┊ GC (min … max): 0.00% … 0.00%
     Time  (median):     20.782 ns               ┊ GC (median):    0.00%
     Time  (mean ± σ):   21.029 ns ±   2.449 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%

     ▂  ▄  █                                                   ▁
     ▇▃▁█▄▁█▅▄█▄▄▁▄▄▅▄▆▆▆▅▅▄▄▃▄▃▁▃▄▁▃▁▁▃▄▃▅▄▇▆▆▆▅▄▆▄▄▄▄▃▄▄▄▄▃▄▄▄▃ █
     18.8 ns       Histogram: log(frequency) by time      31.9 ns <

     Memory estimate: 0 bytes, allocs estimate: 0.

     Now we're approaching jl vectors. 20x is too much gain, for a small
     change... Of course a lot of stuff is broken at the moment.
**** arraymap assoc
     julia> @benchmark assoc(m, 2, 2)
     BenchmarkTools.Trial: 10000 samples with 992 evaluations.
     Range (min … max):  37.541 ns … 626.347 ns  ┊ GC (min … max): 0.00% … 87.17%
     Time  (median):     39.153 ns               ┊ GC (median):    0.00%
     Time  (mean ± σ):   40.560 ns ±  21.497 ns  ┊ GC (mean ± σ):  2.45% ±  4.31%

     ▆█▂
     ▂▄▄▅█████▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂ ▃
     37.5 ns         Histogram: frequency by time         51.5 ns <

     Memory estimate: 112 bytes, allocs estimate: 2.

     julia> @benchmark assoc(m, 17, 18)
     BenchmarkTools.Trial: 10000 samples with 990 evaluations.
     Range (min … max):  43.909 ns … 613.905 ns  ┊ GC (min … max): 0.00% … 82.51%
     Time  (median):     46.486 ns               ┊ GC (median):    0.00%
     Time  (mean ± σ):   49.248 ns ±  28.114 ns  ┊ GC (mean ± σ):  3.62% ±  5.86%

     ▄█▇▂▁  ▁▂                                                    ▂
     █████▇▆███▇▅▆▆▄▆▃▅▅▁▃▄▄▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▆ █
     43.9 ns       Histogram: log(frequency) by time       127 ns <

     Memory estimate: 224 bytes, allocs estimate: 2.

     julia> @benchmark assoc(m, 3, 5)
     BenchmarkTools.Trial: 10000 samples with 991 evaluations.
     Range (min … max):  41.161 ns … 544.153 ns  ┊ GC (min … max): 0.00% … 81.18%
     Time  (median):     43.552 ns               ┊ GC (median):    0.00%
     Time  (mean ± σ):   46.014 ns ±  23.614 ns  ┊ GC (mean ± σ):  3.14% ±  5.63%

     ▅█▆▁▁     ▁▁                                                ▁
     ███████▆▅▅▆███▇▆▆▆▆▅▅▅▅▅▅▆▄▅▅▅▁▄▄▃▄▁▃▁▃▁▄▄▄▁▁▄▄▄▄▁▁▁▃▁▁▁▁▁▁▃ █
     41.2 ns       Histogram: log(frequency) by time      96.2 ns <

     Memory estimate: 208 bytes, allocs estimate: 2.

     julia> @benchmark assoc(m, 15, 11)
     BenchmarkTools.Trial: 10000 samples with 989 evaluations.
     Range (min … max):  47.492 ns … 578.540 ns  ┊ GC (min … max): 0.00% … 73.07%
     Time  (median):     49.950 ns               ┊ GC (median):    0.00%
     Time  (mean ± σ):   54.261 ns ±  32.212 ns  ┊ GC (mean ± σ):  4.39% ±  7.08%

     ▇█▂▁ ▂                                                       ▁
     ███████▆▆▅▅▅▄▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▃▅▆▆█▇ █
     47.5 ns       Histogram: log(frequency) by time       179 ns <

     Memory estimate: 352 bytes, allocs estimate: 2.

     julia> @benchmark assoc(m, :a, 0)
     ERROR: StackOverflowError: Type upgrades not yet implemented.

     Again a suspicious level of improvement, almost O(1), which is absurd.

     Now that could mean that my previous impl was just bad, which is
     possible. I'm still learning the language, after all.
**** arraymap merge
     julia> @benchmark merge(m1, m2)
     BenchmarkTools.Trial: 10000 samples with 10 evaluations.
     Range (min … max):  1.549 μs … 198.823 μs  ┊ GC (min … max): 0.00% … 97.83%
     Time  (median):     1.647 μs               ┊ GC (median):    0.00%
     Time  (mean ± σ):   1.977 μs ±   4.571 μs  ┊ GC (mean ± σ):  5.55% ±  2.39%

     ▂▆██▇▅▂                    ▁▁  ▁▂▂▁ ▁▂▂▂▁ ▁▂▃▁  ▁▁▁▁        ▂
     ████████▅▅▆▆▆▆▆▆▆▄▄▅▇▇█▇▆████▇▇█████████████████████▇▇▇█▇▇▆ █
     1.55 μs      Histogram: log(frequency) by time      3.15 μs <

     Memory estimate: 2.84 KiB, allocs estimate: 43.

     julia> @benchmark merge(m1, m3)
     BenchmarkTools.Trial: 10000 samples with 10 evaluations.
     Range (min … max):  1.307 μs … 253.757 μs  ┊ GC (min … max): 0.00% … 98.43%
     Time  (median):     1.402 μs               ┊ GC (median):    0.00%
     Time  (mean ± σ):   1.658 μs ±   4.784 μs  ┊ GC (mean ± σ):  5.69% ±  1.97%

     ▃▆██▆▄▂                  ▁     ▂▂▁   ▁▂▃▂▁   ▂▂▂           ▂
     ▇████████▆▄▅▄▄▅▄▅▅▇▅▄▄▂▄▅██▇▅▅▅█████▆▆████████████▇▆▇███▇▄▅ █
     1.31 μs      Histogram: log(frequency) by time       2.6 μs <

     Memory estimate: 2.08 KiB, allocs estimate: 38.
**** hashmap get
     We get a small improvement here as well. Likely due to fewer loads.

     julia> @benchmark get(m, 1)
     BenchmarkTools.Trial: 10000 samples with 540 evaluations.
     Range (min … max):  209.781 ns …  1.502 μs  ┊ GC (min … max): 0.00% … 77.25%
     Time  (median):     214.631 ns              ┊ GC (median):    0.00%
     Time  (mean ± σ):   217.030 ns ± 35.088 ns  ┊ GC (mean ± σ):  0.51% ±  2.64%

     ▁▃▃▅▆▇████▇▅▃▁     ▁▁▁                   ▁▂▂▁▁▁▁▁        ▃
     ▂▂▅█████████████████▇██████▇▇▇▇▄▅▄▄▃▃▄▃▄▅▄▆▆▇█████████▇███▇▇ █
     210 ns        Histogram: log(frequency) by time       234 ns <

     Memory estimate: 96 bytes, allocs estimate: 4.

     julia> @benchmark get(m, 31)
     BenchmarkTools.Trial: 10000 samples with 560 evaluations.
     Range (min … max):  207.234 ns …  1.605 μs  ┊ GC (min … max): 0.00% … 84.26%
     Time  (median):     214.994 ns              ┊ GC (median):    0.00%
     Time  (mean ± σ):   218.012 ns ± 41.303 ns  ┊ GC (mean ± σ):  0.60% ±  2.74%

     ▁▂██▆▃▄▃
     ▂▁▁▁▂▂▃▃▄▇██████████▇▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂ ▃
     207 ns          Histogram: frequency by time          238 ns <

     Memory estimate: 96 bytes, allocs estimate: 4.

     Note, I haven't gone back and rewritten hashmap get to not allocate.
*** C style merge impl
    Very dumb, but easy for compilers:

    m = into(emptymap, partition(2), 1:32)
    m1 = into(emptymap, partition(2), 1:16)
    m2 = into(emptymap, partition(2), 17:32)
    m3 = into(emptymap, partition(2), 9:22)

    julia> @benchmark merge(m1, m1)
    BenchmarkTools.Trial: 10000 samples with 954 evaluations.
    Range (min … max):   95.862 ns … 582.558 ns  ┊ GC (min … max): 0.00% … 68.26%
    Time  (median):     100.159 ns               ┊ GC (median):    0.00%
    Time  (mean ± σ):   106.289 ns ±  39.240 ns  ┊ GC (mean ± σ):  3.29% ±  7.62%

    █▇▄▁                                                          ▁
    ████▇▄▃▄▄▄▃▃▃▅▄▃▅▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆▇▇▅▄▃▃▁▁▃▁▃▁▁▃▁▁▃▁▃▁▁▃▅▆ █
    95.9 ns       Histogram: log(frequency) by time        415 ns <

    Memory estimate: 544 bytes, allocs estimate: 3.

    julia> @benchmark merge(m, m)
    BenchmarkTools.Trial: 10000 samples with 832 evaluations.
    Range (min … max):  148.862 ns … 713.871 ns  ┊ GC (min … max): 0.00% … 57.10%
    Time  (median):     153.909 ns               ┊ GC (median):    0.00%
    Time  (mean ± σ):   161.659 ns ±  46.506 ns  ┊ GC (mean ± σ):  3.39% ±  8.29%

    █▆▄▃▂                                                         ▁
    █████▇▅▃▄▅▄▃▁▁▁▁▄▄▃▅▃▅▄▄▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▅▇█ █
    149 ns        Histogram: log(frequency) by time        499 ns <

    Memory estimate: 928 bytes, allocs estimate: 3.

    julia> @benchmark merge(m1, m3)
    BenchmarkTools.Trial: 10000 samples with 943 evaluations.
    Range (min … max):  103.745 ns … 670.425 ns  ┊ GC (min … max): 0.00% … 56.68%
    Time  (median):     108.247 ns               ┊ GC (median):    0.00%
    Time  (mean ± σ):   112.871 ns ±  35.509 ns  ┊ GC (mean ± σ):  3.16% ±  7.60%

    █▆▄▁                                                          ▁
    ████▇▄▄▃▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▃▄▃▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▃▃▆ █
    104 ns        Histogram: log(frequency) by time        424 ns <

    Memory estimate: 560 bytes, allocs estimate: 3.

    julia> @benchmark merge(m1, m2)
    BenchmarkTools.Trial: 10000 samples with 966 evaluations.
    Range (min … max):  81.285 ns … 583.220 ns  ┊ GC (min … max): 0.00% … 65.82%
    Time  (median):     83.667 ns               ┊ GC (median):    0.00%
    Time  (mean ± σ):   88.010 ns ±  31.063 ns  ┊ GC (mean ± σ):  2.58% ±  6.44%

    ██▃ ▃▃▁                                                      ▂
    ███████▇▄▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇█▇▆ █
    81.3 ns       Histogram: log(frequency) by time       215 ns <

    Memory estimate: 352 bytes, allocs estimate: 2.

    julia> @benchmark merge(m, m3)
    BenchmarkTools.Trial: 10000 samples with 909 evaluations.
    Range (min … max):  122.763 ns … 633.378 ns  ┊ GC (min … max): 0.00% … 64.58%
    Time  (median):     126.142 ns               ┊ GC (median):    0.00%
    Time  (mean ± σ):   133.789 ns ±  47.043 ns  ┊ GC (mean ± σ):  3.75% ±  8.52%

    █▄▄                                                           ▁
    ████▇▄▄▁▃▄▄▃▄▄▁▁▃▁▃▃▃▅▁▃▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▄▆▆▄▄▄▅▇▇ █
    123 ns        Histogram: log(frequency) by time        459 ns <

    Memory estimate: 800 bytes, allocs estimate: 3.

    julia> @benchmark merge(m1, m2, m3)
    BenchmarkTools.Trial: 10000 samples with 334 evaluations.
    Range (min … max):  263.566 ns …   2.741 μs  ┊ GC (min … max): 0.00% … 87.55%
    Time  (median):     278.329 ns               ┊ GC (median):    0.00%
    Time  (mean ± σ):   292.044 ns ± 130.735 ns  ┊ GC (mean ± σ):  3.96% ±  7.42%

    ▂▆█▇█▅▃▁▂▂▃▂▁                                                 ▂
    ██████████████▇▆▅▆▆▆▅▅▃▃▃▄▄▄▃▁▁▁▃▁▁▁▁▁▁▁▃▄▃▁▁▁▄▁▃▁▃▁▃▁▃▃▃▃▃▄▃ █
    264 ns        Histogram: log(frequency) by time        500 ns <

    Memory estimate: 1.17 KiB, allocs estimate: 6.

   Huge improvement (>20x). Curiously, using iterators, reduce, and vcat got me
   only a 2x improvement. There's a lot of allocation going on there.
* Ideas / devlog
** IO and events
   Something I really want to work out is having all IO being either input or
   output streams. That is completely decouple programs from their IO so that it
   can be stubbed, sandboxed, etc..

   Extending transducers to channels is a natural way to do that to some extent,
   but there are some corner cases I need to worry about.

   Events will come in ludicrous quantities. They can't be buffered for any
   length of time.

   Any stream to stream process needs the result stream to be eagerly consumed
   otherwise we're going to have spacetime leakage.

   However a reduction over these streams makes for a natural reactive value.

   I can force that by not defining an "empty stream" which you can pass to
   =into= as a first argument. Thus you'll be able to call transduce on streams,
   but you'll ~need~ to define a reducing function.

   That might be enough.

   Stream combinators is also a bit of a mess. Streams don't have a natural
   stride like sequences so you don't necessarily want to wait for a new value
   from each input stream before moving on.

   In fact, you can't in general because one stream might produce terrabytes of
   data before another emits a single event.

   We can leave it up to the developer to write their own function to combine
   streams. If they want to buffer the fast ones they can, but it's application
   logic.

   I don't want to get trapped in Hoare's purgatory of a dozen interleaving
   operators.

   So we say how streams are combined is entirely up to the developer, maybe
   there will be useful patterns to write helpers for, but in the end the
   transduction takes a single stream to a reactive value.

   If you're sure the channel will close you can dump it into a collection, but
   if it doesn't close, that will park and never return.

   I think I need some way to name intermediate streams since I want to combine
   them in complex ways.

   If channels apply back pressure, maybe it's okay to dump into channels.
